{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.parse\n",
    "import re\n",
    "\n",
    "def extract_features(url):\n",
    "    # Parse the URL\n",
    "    parsed_url = urllib.parse.urlparse(url)\n",
    "    \n",
    "    # Features\n",
    "    features = {\n",
    "        \"NumDots\": url.count('.'),\n",
    "        \"SubdomainLevel\": len(parsed_url.hostname.split('.')),\n",
    "        \"PathLevel\": url.count('/'),\n",
    "        \"UrlLength\": len(url),\n",
    "        \"NumDash\": url.count('-'),\n",
    "        \"NumDashInHostname\": parsed_url.hostname.count('-'),\n",
    "        \"AtSymbol\": 1 if '@' in url else 0,\n",
    "        \"TildeSymbol\": 1 if '~' in url else 0,\n",
    "        \"NumUnderscore\": url.count('_'),\n",
    "        \"NumPercent\": url.count('%'),\n",
    "\n",
    "        \"NumQueryComponents\": len(urllib.parse.parse_qs(parsed_url.query)),\n",
    "        \"NumAmpersand\": url.count('&'),\n",
    "        \"NumHash\": url.count('#'),\n",
    "        \"NumNumericChars\": sum(c.isdigit() for c in url),\n",
    "        \"NoHttps\": 1 if parsed_url.scheme != 'https' else 0,\n",
    "        \n",
    "        \"RandomString\": 1 if re.search(r'\\b[0-9a-f]{10}\\b', url) else 0, # Assuming random string contains 10 hex characters\n",
    "        \"IpAddress\": 1 if re.match(r'\\b\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\b', parsed_url.netloc) else 0,\n",
    "        \"DomainInSubdomains\": 1 if parsed_url.netloc.count('.') > 2 else 0,\n",
    "        \"DomainInPaths\": 1 if '.' in parsed_url.path else 0,\n",
    "        \"HttpsInHostname\": 1 if 'https' in parsed_url.netloc else 0,\n",
    "\n",
    "        \"HostnameLength\": len(parsed_url.hostname),\n",
    "        \"PathLength\": len(parsed_url.path),\n",
    "        \"QueryLength\": len(parsed_url.query),\n",
    "        \"DoubleSlashInPath\": 1 if '//' in url else 0,\n",
    "    }\n",
    "    \n",
    "    return features\n",
    "\n",
    "# Example usage\n",
    "url = \"https://www.example.com/page?param1=value1&param2=value2\"\n",
    "features = extract_features(url)\n",
    "print(features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'NumDots': 2, 'SubdomainLevel': 3, 'PathLevel': 2, 'UrlLength': 22, 'NumDash': 0, 'NumDashInHostname': 0, 'AtSymbol': 0, 'TildeSymbol': 0, 'NumUnderscore': 0, 'NumPercent': 0, 'NumQueryComponents': 0, 'NumAmpersand': 0, 'NumHash': 0, 'NumNumericChars': 0, 'NoHttps': 0, 'RandomString': 0, 'IpAddress': 0, 'DomainInSubdomains': 0, 'DomainInPaths': 0, 'HttpsInHostname': 0, 'HostnameLength': 14, 'PathLength': 0, 'QueryLength': 0, 'DoubleSlashInPath': 1, 'NumSensitiveWords': 0, 'EmbeddedBrandName': 1, 'PctExtHyperlinks': 70.0, 'PctExtResourceUrls': 0.0, 'ExtFavicon': 0, 'InsecureForms': False}\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "def extract_features(url):\n",
    "    # Function to parse HTML content of the webpage and extract required features\n",
    "    def parse_html_content(html_content):\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "        \n",
    "        \n",
    "        # Count total number of hyperlinks and external hyperlinks\n",
    "        total_hyperlinks = len(soup.find_all('a'))\n",
    "        ext_hyperlinks = sum(1 for link in soup.find_all('a') if 'http' in link.get('href'))\n",
    "        pct_ext_hyperlinks = (ext_hyperlinks / total_hyperlinks) * 100 if total_hyperlinks > 0 else 0\n",
    "        \n",
    "        # Count total number of resource URLs and external resource URLs (like images, scripts)\n",
    "        total_resources = len(soup.find_all(['img', 'script', 'link']))\n",
    "        ext_resources = sum(1 for res in soup.find_all(['img', 'script', 'link']) if 'http' in res.get('src', '') or 'http' in res.get('href', ''))\n",
    "        pct_ext_resources = (ext_resources / total_resources) * 100 if total_resources > 0 else 0\n",
    "        \n",
    "        # Check if the webpage has an external favicon\n",
    "        favicon_url = soup.find('link', rel='shortcut icon')\n",
    "        ext_favicon = 1 if favicon_url and 'http' in favicon_url.get('href', '') else 0\n",
    "        \n",
    "        # Check if the webpage contains insecure forms\n",
    "        insecure_forms = any(form.get('action', '').startswith('http://') for form in soup.find_all('form'))\n",
    "        \n",
    "        return pct_ext_hyperlinks, pct_ext_resources, ext_favicon, insecure_forms\n",
    "    \n",
    "    # Parse the URL\n",
    "    parsed_url = urllib.parse.urlparse(url)\n",
    "\n",
    "    # Predefined lists of sensitive words and brand names\n",
    "    sensitive_words = [\"login\", \"password\", \"banking\", \"account\", \"verify\", \"secure\"]\n",
    "    brand_names = [\"paypal\", \"google\", \"facebook\", \"amazon\", \"apple\"]\n",
    "\n",
    "    # Function to count occurrences of sensitive words and check for embedded brand names\n",
    "    def count_sensitive_words(url):\n",
    "        num_sensitive_words = sum(1 for word in sensitive_words if word in url.lower())\n",
    "        embedded_brand_name = any(brand in url.lower() for brand in brand_names)\n",
    "        return num_sensitive_words, embedded_brand_name\n",
    "    \n",
    "    # Extract the features\n",
    "    num_sensitive_words, embedded_brand_name = count_sensitive_words(url)\n",
    "    \n",
    "    # Fetch the webpage content\n",
    "    try:\n",
    "        with urllib.request.urlopen(url) as response:\n",
    "            html_content = response.read()\n",
    "            pct_ext_hyperlinks, pct_ext_resources, ext_favicon, insecure_forms = parse_html_content(html_content)\n",
    "    except Exception as e:\n",
    "        print(\"Error fetching webpage:\", e)\n",
    "        pct_ext_hyperlinks, pct_ext_resources, ext_favicon, insecure_forms = 0, 0, 0, 0\n",
    "    \n",
    "    # Features\n",
    "    features = {\n",
    "        \"NumDots\": url.count('.'),\n",
    "        \"SubdomainLevel\": len(parsed_url.hostname.split('.')),\n",
    "        \"PathLevel\": url.count('/'),\n",
    "        \"UrlLength\": len(url),\n",
    "        \"NumDash\": url.count('-'),\n",
    "        \"NumDashInHostname\": parsed_url.hostname.count('-'),\n",
    "        \"AtSymbol\": 1 if '@' in url else 0,\n",
    "        \"TildeSymbol\": 1 if '~' in url else 0,\n",
    "        \"NumUnderscore\": url.count('_'),\n",
    "        \"NumPercent\": url.count('%'),\n",
    "        \n",
    "        \"NumQueryComponents\": len(urllib.parse.parse_qs(parsed_url.query)),\n",
    "        \"NumAmpersand\": url.count('&'),\n",
    "        \"NumHash\": url.count('#'),\n",
    "        \"NumNumericChars\": sum(c.isdigit() for c in url),\n",
    "        \"NoHttps\": 1 if parsed_url.scheme != 'https' else 0,\n",
    "        \n",
    "        \"RandomString\": 1 if re.search(r'\\b[0-9a-f]{10}\\b', url) else 0, # Assuming random string contains 10 hex characters\n",
    "        \"IpAddress\": 1 if re.match(r'\\b\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\b', parsed_url.netloc) else 0,\n",
    "        \"DomainInSubdomains\": 1 if parsed_url.netloc.count('.') > 2 else 0,\n",
    "        \"DomainInPaths\": 1 if '.' in parsed_url.path else 0,\n",
    "        \"HttpsInHostname\": 1 if 'https' in parsed_url.netloc else 0,\n",
    "\n",
    "        \"HostnameLength\": len(parsed_url.hostname),\n",
    "        \"PathLength\": len(parsed_url.path),\n",
    "        \"QueryLength\": len(parsed_url.query),\n",
    "        \"DoubleSlashInPath\": 1 if '//' in url else 0,\n",
    "        \n",
    "        \"NumSensitiveWords\": num_sensitive_words,\n",
    "        \"EmbeddedBrandName\": 1 if embedded_brand_name else 0,\n",
    "\n",
    "        \"PctExtHyperlinks\": pct_ext_hyperlinks,\n",
    "        \"PctExtResourceUrls\": pct_ext_resources,\n",
    "        \"ExtFavicon\": ext_favicon,\n",
    "        \"InsecureForms\": insecure_forms,\n",
    "    }\n",
    "    \n",
    "    return features\n",
    "\n",
    "# Example usage\n",
    "url = \"https://www.google.com\"\n",
    "features = extract_features(url)\n",
    "print(features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'RelativeFormAction': 100.0, 'ExtFormAction': 0.0, 'AbnormalFormAction': 0.0, 'PctNullSelfRedirectHyperlinks': 0.0}\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def extract_features(url):\n",
    "    # Function to parse HTML content of the webpage and extract required features\n",
    "    def parse_html_content(html_content):\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "        \n",
    "        forms = soup.find_all('form')\n",
    "        links = soup.find_all('a')\n",
    "        \n",
    "        total_forms = len(forms)\n",
    "        total_links = len(links)\n",
    "        \n",
    "        # Initialize feature counts\n",
    "        relative_form_action_count = 0\n",
    "        ext_form_action_count = 0\n",
    "        abnormal_form_action_count = 0\n",
    "        null_self_redirect_count = 0\n",
    "        \n",
    "        for form in forms:\n",
    "            action = form.get('action', '')\n",
    "            if action.startswith('/'):\n",
    "                relative_form_action_count += 1\n",
    "            elif '://' in action:\n",
    "                ext_form_action_count += 1\n",
    "            elif action and not action.startswith('#') and not action.startswith('javascript:'):\n",
    "                abnormal_form_action_count += 1\n",
    "        \n",
    "        for link in links:\n",
    "            href = link.get('href', '')\n",
    "            if not href or href.startswith('#') or href.lower() in ['null', 'void(0)']:\n",
    "                null_self_redirect_count += 1\n",
    "        \n",
    "        # Calculate percentages\n",
    "        pct_relative_form_action = (relative_form_action_count / total_forms) * 100 if total_forms > 0 else 0\n",
    "        pct_ext_form_action = (ext_form_action_count / total_forms) * 100 if total_forms > 0 else 0\n",
    "        pct_abnormal_form_action = (abnormal_form_action_count / total_forms) * 100 if total_forms > 0 else 0\n",
    "        pct_null_self_redirect = (null_self_redirect_count / total_links) * 100 if total_links > 0 else 0\n",
    "        \n",
    "        return pct_relative_form_action, pct_ext_form_action, pct_abnormal_form_action, pct_null_self_redirect\n",
    "    \n",
    "    # Fetch the webpage content\n",
    "    try:\n",
    "        with urllib.request.urlopen(url) as response:\n",
    "            html_content = response.read()\n",
    "            pct_relative_form_action, pct_ext_form_action, pct_abnormal_form_action, pct_null_self_redirect = parse_html_content(html_content)\n",
    "    except Exception as e:\n",
    "        print(\"Error fetching webpage:\", e)\n",
    "        pct_relative_form_action, pct_ext_form_action, pct_abnormal_form_action, pct_null_self_redirect = 0, 0, 0, 0\n",
    "    \n",
    "    # Features\n",
    "    features = {\n",
    "        \"RelativeFormAction\": pct_relative_form_action,\n",
    "        \"ExtFormAction\": pct_ext_form_action,\n",
    "        \"AbnormalFormAction\": pct_abnormal_form_action,\n",
    "        \"PctNullSelfRedirectHyperlinks\": pct_null_self_redirect\n",
    "    }\n",
    "    \n",
    "    return features\n",
    "\n",
    "# Example usage\n",
    "url = \"https://www.google.com\"\n",
    "features = extract_features(url)\n",
    "print(features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'NumDots': 2, 'SubdomainLevel': 3, 'PathLevel': 2, 'UrlLength': 22, 'NumDash': 0, 'NumDashInHostname': 0, 'AtSymbol': 0, 'TildeSymbol': 0, 'NumUnderscore': 0, 'NumPercent': 0, 'NumQueryComponents': 0, 'NumAmpersand': 0, 'NumHash': 0, 'NumNumericChars': 0, 'NoHttps': 0, 'RandomString': 0, 'IpAddress': 0, 'DomainInSubdomains': 0, 'DomainInPaths': 0, 'HttpsInHostname': 0, 'HostnameLength': 14, 'PathLength': 0, 'QueryLength': 0, 'DoubleSlashInPath': 1, 'NumSensitiveWords': 0, 'EmbeddedBrandName': 1, 'PctExtHyperlinks': 70.0, 'PctExtResourceUrls': 0.0, 'ExtFavicon': 0, 'InsecureForms': False, 'RelativeFormAction': 100.0, 'ExtFormAction': 0.0, 'AbnormalFormAction': 0.0, 'PctNullSelfRedirectHyperlinks': 0.0}\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "def extract_features(url):\n",
    "    # Function to parse HTML content of the webpage and extract required features\n",
    "    def parse_html_content(html_content):\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "        \n",
    "        forms = soup.find_all('form')\n",
    "        links = soup.find_all('a')\n",
    "        \n",
    "        total_forms = len(forms)\n",
    "        total_links = len(links)\n",
    "        \n",
    "        # Initialize feature counts\n",
    "        relative_form_action_count = 0\n",
    "        ext_form_action_count = 0\n",
    "        abnormal_form_action_count = 0\n",
    "        null_self_redirect_count = 0\n",
    "        \n",
    "        for form in forms:\n",
    "            action = form.get('action', '')\n",
    "            if action.startswith('/'):\n",
    "                relative_form_action_count += 1\n",
    "            elif '://' in action:\n",
    "                ext_form_action_count += 1\n",
    "            elif action and not action.startswith('#') and not action.startswith('javascript:'):\n",
    "                abnormal_form_action_count += 1\n",
    "        \n",
    "        for link in links:\n",
    "            href = link.get('href', '')\n",
    "            if not href or href.startswith('#') or href.lower() in ['null', 'void(0)']:\n",
    "                null_self_redirect_count += 1\n",
    "        \n",
    "        # Calculate percentages\n",
    "        pct_relative_form_action = (relative_form_action_count / total_forms) * 100 if total_forms > 0 else 0\n",
    "        pct_ext_form_action = (ext_form_action_count / total_forms) * 100 if total_forms > 0 else 0\n",
    "        pct_abnormal_form_action = (abnormal_form_action_count / total_forms) * 100 if total_forms > 0 else 0\n",
    "        pct_null_self_redirect = (null_self_redirect_count / total_links) * 100 if total_links > 0 else 0\n",
    "        \n",
    "        # Count total number of hyperlinks and external hyperlinks\n",
    "        total_hyperlinks = len(links)\n",
    "        ext_hyperlinks = sum(1 for link in links if 'http' in link.get('href'))\n",
    "        pct_ext_hyperlinks = (ext_hyperlinks / total_hyperlinks) * 100 if total_hyperlinks > 0 else 0\n",
    "        \n",
    "        # Count total number of resource URLs and external resource URLs (like images, scripts)\n",
    "        total_resources = len(soup.find_all(['img', 'script', 'link']))\n",
    "        ext_resources = sum(1 for res in soup.find_all(['img', 'script', 'link']) if 'http' in res.get('src', '') or 'http' in res.get('href', ''))\n",
    "        pct_ext_resources = (ext_resources / total_resources) * 100 if total_resources > 0 else 0\n",
    "        \n",
    "        # Check if the webpage has an external favicon\n",
    "        favicon_url = soup.find('link', rel='shortcut icon')\n",
    "        ext_favicon = 1 if favicon_url and 'http' in favicon_url.get('href', '') else 0\n",
    "        \n",
    "        # Check if the webpage contains insecure forms\n",
    "        insecure_forms = any(form.get('action', '').startswith('http://') for form in forms)\n",
    "        \n",
    "        return pct_relative_form_action, pct_ext_form_action, pct_abnormal_form_action, pct_null_self_redirect, pct_ext_hyperlinks, pct_ext_resources, ext_favicon, insecure_forms\n",
    "    \n",
    "    # Parse the URL\n",
    "    parsed_url = urllib.parse.urlparse(url)\n",
    "\n",
    "    # Predefined lists of sensitive words and brand names\n",
    "    sensitive_words = [\"login\", \"password\", \"banking\", \"account\", \"verify\", \"secure\"]\n",
    "    brand_names = [\"paypal\", \"google\", \"facebook\", \"amazon\", \"apple\"]\n",
    "\n",
    "    # Function to count occurrences of sensitive words and check for embedded brand names\n",
    "    def count_sensitive_words(url):\n",
    "        num_sensitive_words = sum(1 for word in sensitive_words if word in url.lower())\n",
    "        embedded_brand_name = any(brand in url.lower() for brand in brand_names)\n",
    "        return num_sensitive_words, embedded_brand_name\n",
    "    \n",
    "    # Extract the features\n",
    "    num_sensitive_words, embedded_brand_name = count_sensitive_words(url)\n",
    "    \n",
    "    # Fetch the webpage content\n",
    "    try:\n",
    "        with urllib.request.urlopen(url) as response:\n",
    "            html_content = response.read()\n",
    "            pct_relative_form_action, pct_ext_form_action, pct_abnormal_form_action, pct_null_self_redirect, pct_ext_hyperlinks, pct_ext_resources, ext_favicon, insecure_forms = parse_html_content(html_content)\n",
    "    except Exception as e:\n",
    "        print(\"Error fetching webpage:\", e)\n",
    "        pct_relative_form_action, pct_ext_form_action, pct_abnormal_form_action, pct_null_self_redirect, pct_ext_hyperlinks, pct_ext_resources, ext_favicon, insecure_forms = 0, 0, 0, 0, 0, 0, 0, 0\n",
    "    \n",
    "    # Features\n",
    "    features = {\n",
    "        \"NumDots\": url.count('.'),\n",
    "        \"SubdomainLevel\": len(parsed_url.hostname.split('.')),\n",
    "        \"PathLevel\": url.count('/'),\n",
    "        \"UrlLength\": len(url),\n",
    "        \"NumDash\": url.count('-'),\n",
    "        \"NumDashInHostname\": parsed_url.hostname.count('-'),\n",
    "        \"AtSymbol\": 1 if '@' in url else 0,\n",
    "        \"TildeSymbol\": 1 if '~' in url else 0,\n",
    "        \"NumUnderscore\": url.count('_'),\n",
    "        \"NumPercent\": url.count('%'),\n",
    "        \n",
    "        \"NumQueryComponents\": len(urllib.parse.parse_qs(parsed_url.query)),\n",
    "        \"NumAmpersand\": url.count('&'),\n",
    "        \"NumHash\": url.count('#'),\n",
    "        \"NumNumericChars\": sum(c.isdigit() for c in url),\n",
    "        \"NoHttps\": 1 if parsed_url.scheme != 'https' else 0,\n",
    "        \n",
    "        \"RandomString\": 1 if re.search(r'\\b[0-9a-f]{10}\\b', url) else 0, # Assuming random string contains 10 hex characters\n",
    "        \"IpAddress\": 1 if re.match(r'\\b\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\b', parsed_url.netloc) else 0,\n",
    "        \"DomainInSubdomains\": 1 if parsed_url.netloc.count('.') > 2 else 0,\n",
    "        \"DomainInPaths\": 1 if '.' in parsed_url.path else 0,\n",
    "        \"HttpsInHostname\": 1 if 'https' in parsed_url.netloc else 0,\n",
    "\n",
    "        \"HostnameLength\": len(parsed_url.hostname),\n",
    "        \"PathLength\": len(parsed_url.path),\n",
    "        \"QueryLength\": len(parsed_url.query),\n",
    "        \"DoubleSlashInPath\": 1 if '//' in url else 0,\n",
    "        \n",
    "        \"NumSensitiveWords\": num_sensitive_words,\n",
    "        \"EmbeddedBrandName\": 1 if embedded_brand_name else 0,\n",
    "\n",
    "        \"PctExtHyperlinks\": pct_ext_hyperlinks,\n",
    "        \"PctExtResourceUrls\": pct_ext_resources,\n",
    "        \"ExtFavicon\": ext_favicon,\n",
    "        \"InsecureForms\": insecure_forms,\n",
    "\n",
    "        \"RelativeFormAction\": pct_relative_form_action,\n",
    "        \"ExtFormAction\": pct_ext_form_action,\n",
    "        \"AbnormalFormAction\": pct_abnormal_form_action,\n",
    "        \"PctNullSelfRedirectHyperlinks\": pct_null_self_redirect\n",
    "    }\n",
    "    \n",
    "    return features\n",
    "\n",
    "# Example usage\n",
    "url = \"https://www.google.com\"\n",
    "features = extract_features(url)\n",
    "print(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'FrequentDomainNameMismatch': 1, 'FakeLinkInStatusBar': 0, 'RightClickDisabled': 0, 'PopUpWindow': 0}\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def extract_features(url):\n",
    "    # Function to parse HTML content of the webpage and extract required features\n",
    "    def parse_html_content(html_content):\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "        \n",
    "        # Check for frequent domain name mismatch\n",
    "        domain = urllib.parse.urlparse(url).netloc\n",
    "        mismatch_count = sum(1 for link in soup.find_all('a') if domain not in link.get('href'))\n",
    "        frequent_domain_name_mismatch = 1 if mismatch_count > 5 else 0\n",
    "        \n",
    "        # Check for fake links in the status bar\n",
    "        fake_links = sum(1 for link in soup.find_all('a') if 'javascript' in link.get('href', '').lower())\n",
    "        fake_link_in_status_bar = 1 if fake_links > 0 else 0\n",
    "        \n",
    "        # Check if right-click is disabled\n",
    "        right_click_disabled = 1 if 'contextmenu' in html_content.decode().lower() else 0\n",
    "        \n",
    "        # Check for pop-up windows\n",
    "        pop_up_windows = sum(1 for tag in soup.find_all() if tag.get('onload') and 'window.open' in tag.get('onload').lower())\n",
    "        pop_up_window = 1 if pop_up_windows > 0 else 0\n",
    "        \n",
    "        return frequent_domain_name_mismatch, fake_link_in_status_bar, right_click_disabled, pop_up_window\n",
    "    \n",
    "    # Fetch the webpage content\n",
    "    try:\n",
    "        with urllib.request.urlopen(url) as response:\n",
    "            html_content = response.read()\n",
    "            frequent_domain_name_mismatch, fake_link_in_status_bar, right_click_disabled, pop_up_window = parse_html_content(html_content)\n",
    "    except Exception as e:\n",
    "        print(\"Error fetching webpage:\", e)\n",
    "        frequent_domain_name_mismatch, fake_link_in_status_bar, right_click_disabled, pop_up_window = 0, 0, 0, 0\n",
    "    \n",
    "    # Features\n",
    "    features = {\n",
    "        \"FrequentDomainNameMismatch\": frequent_domain_name_mismatch,\n",
    "        \"FakeLinkInStatusBar\": fake_link_in_status_bar,\n",
    "        \"RightClickDisabled\": right_click_disabled,\n",
    "        \"PopUpWindow\": pop_up_window\n",
    "    }\n",
    "    \n",
    "    return features\n",
    "\n",
    "# Example usage\n",
    "url = \"https://www.google.com\"\n",
    "features = extract_features(url)\n",
    "print(features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'NumDots': 2, 'SubdomainLevel': 3, 'PathLevel': 2, 'UrlLength': 22, 'NumDash': 0, 'NumDashInHostname': 0, 'AtSymbol': 0, 'TildeSymbol': 0, 'NumUnderscore': 0, 'NumPercent': 0, 'NumQueryComponents': 0, 'NumAmpersand': 0, 'NumHash': 0, 'NumNumericChars': 0, 'NoHttps': 0, 'RandomString': 0, 'IpAddress': 0, 'DomainInSubdomains': 0, 'DomainInPaths': 0, 'HttpsInHostname': 0, 'HostnameLength': 14, 'PathLength': 0, 'QueryLength': 0, 'DoubleSlashInPath': 1, 'NumSensitiveWords': 0, 'EmbeddedBrandName': 1, 'PctExtHyperlinks': 70.0, 'PctExtResourceUrls': 0.0, 'ExtFavicon': 0, 'InsecureForms': False, 'RelativeFormAction': 100.0, 'ExtFormAction': 0.0, 'AbnormalFormAction': 0.0, 'PctNullSelfRedirectHyperlinks': 0.0, 'FrequentDomainNameMismatch': 1, 'FakeLinkInStatusBar': 0, 'RightClickDisabled': 0, 'PopUpWindow': 0}\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "def extract_features(url):\n",
    "    # Function to parse HTML content of the webpage and extract required features\n",
    "    def parse_html_content(html_content):\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "        # Check for frequent domain name mismatch\n",
    "        domain = urllib.parse.urlparse(url).netloc\n",
    "        mismatch_count = sum(1 for link in soup.find_all('a') if domain not in link.get('href'))\n",
    "        frequent_domain_name_mismatch = 1 if mismatch_count > 5 else 0\n",
    "        \n",
    "        # Check for fake links in the status bar\n",
    "        fake_links = sum(1 for link in soup.find_all('a') if 'javascript' in link.get('href', '').lower())\n",
    "        fake_link_in_status_bar = 1 if fake_links > 0 else 0\n",
    "        \n",
    "        # Check if right-click is disabled\n",
    "        right_click_disabled = 1 if 'contextmenu' in html_content.decode().lower() else 0\n",
    "        \n",
    "        # Check for pop-up windows\n",
    "        pop_up_windows = sum(1 for tag in soup.find_all() if tag.get('onload') and 'window.open' in tag.get('onload').lower())\n",
    "        pop_up_window = 1 if pop_up_windows > 0 else 0\n",
    "        \n",
    "        forms = soup.find_all('form')\n",
    "        links = soup.find_all('a')\n",
    "        \n",
    "        total_forms = len(forms)\n",
    "        total_links = len(links)\n",
    "        \n",
    "        # Initialize feature counts\n",
    "        relative_form_action_count = 0\n",
    "        ext_form_action_count = 0\n",
    "        abnormal_form_action_count = 0\n",
    "        null_self_redirect_count = 0\n",
    "        \n",
    "        for form in forms:\n",
    "            action = form.get('action', '')\n",
    "            if action.startswith('/'):\n",
    "                relative_form_action_count += 1\n",
    "            elif '://' in action:\n",
    "                ext_form_action_count += 1\n",
    "            elif action and not action.startswith('#') and not action.startswith('javascript:'):\n",
    "                abnormal_form_action_count += 1\n",
    "        \n",
    "        for link in links:\n",
    "            href = link.get('href', '')\n",
    "            if not href or href.startswith('#') or href.lower() in ['null', 'void(0)']:\n",
    "                null_self_redirect_count += 1\n",
    "        \n",
    "        # Calculate percentages\n",
    "        pct_relative_form_action = (relative_form_action_count / total_forms) * 100 if total_forms > 0 else 0\n",
    "        pct_ext_form_action = (ext_form_action_count / total_forms) * 100 if total_forms > 0 else 0\n",
    "        pct_abnormal_form_action = (abnormal_form_action_count / total_forms) * 100 if total_forms > 0 else 0\n",
    "        pct_null_self_redirect = (null_self_redirect_count / total_links) * 100 if total_links > 0 else 0\n",
    "        \n",
    "        # Count total number of hyperlinks and external hyperlinks\n",
    "        total_hyperlinks = len(links)\n",
    "        ext_hyperlinks = sum(1 for link in links if 'http' in link.get('href'))\n",
    "        pct_ext_hyperlinks = (ext_hyperlinks / total_hyperlinks) * 100 if total_hyperlinks > 0 else 0\n",
    "        \n",
    "        # Count total number of resource URLs and external resource URLs (like images, scripts)\n",
    "        total_resources = len(soup.find_all(['img', 'script', 'link']))\n",
    "        ext_resources = sum(1 for res in soup.find_all(['img', 'script', 'link']) if 'http' in res.get('src', '') or 'http' in res.get('href', ''))\n",
    "        pct_ext_resources = (ext_resources / total_resources) * 100 if total_resources > 0 else 0\n",
    "        \n",
    "        # Check if the webpage has an external favicon\n",
    "        favicon_url = soup.find('link', rel='shortcut icon')\n",
    "        ext_favicon = 1 if favicon_url and 'http' in favicon_url.get('href', '') else 0\n",
    "        \n",
    "        # Check if the webpage contains insecure forms\n",
    "        insecure_forms = any(form.get('action', '').startswith('http://') for form in forms)\n",
    "        \n",
    "        return pct_relative_form_action, pct_ext_form_action, pct_abnormal_form_action, pct_null_self_redirect, pct_ext_hyperlinks, pct_ext_resources, ext_favicon, insecure_forms, frequent_domain_name_mismatch, fake_link_in_status_bar, right_click_disabled, pop_up_window\n",
    "    \n",
    "    # Parse the URL\n",
    "    parsed_url = urllib.parse.urlparse(url)\n",
    "\n",
    "    # Predefined lists of sensitive words and brand names\n",
    "    sensitive_words = [\"login\", \"password\", \"banking\", \"account\", \"verify\", \"secure\"]\n",
    "    brand_names = [\"paypal\", \"google\", \"facebook\", \"amazon\", \"apple\"]\n",
    "\n",
    "    # Function to count occurrences of sensitive words and check for embedded brand names\n",
    "    def count_sensitive_words(url):\n",
    "        num_sensitive_words = sum(1 for word in sensitive_words if word in url.lower())\n",
    "        embedded_brand_name = any(brand in url.lower() for brand in brand_names)\n",
    "        return num_sensitive_words, embedded_brand_name\n",
    "    \n",
    "    # Extract the features\n",
    "    num_sensitive_words, embedded_brand_name = count_sensitive_words(url)\n",
    "    \n",
    "    # Fetch the webpage content\n",
    "    try:\n",
    "        with urllib.request.urlopen(url) as response:\n",
    "            html_content = response.read()\n",
    "            pct_relative_form_action, pct_ext_form_action, pct_abnormal_form_action, pct_null_self_redirect, pct_ext_hyperlinks, pct_ext_resources, ext_favicon, insecure_forms, frequent_domain_name_mismatch, fake_link_in_status_bar, right_click_disabled, pop_up_window = parse_html_content(html_content)\n",
    "    except Exception as e:\n",
    "        print(\"Error fetching webpage:\", e)\n",
    "        pct_relative_form_action, pct_ext_form_action, pct_abnormal_form_action, pct_null_self_redirect, pct_ext_hyperlinks, pct_ext_resources, ext_favicon, insecure_forms, frequent_domain_name_mismatch, fake_link_in_status_bar, right_click_disabled, pop_up_window = 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
    "    \n",
    "    # Features\n",
    "    features = {\n",
    "        \"NumDots\": url.count('.'),\n",
    "        \"SubdomainLevel\": len(parsed_url.hostname.split('.')),\n",
    "        \"PathLevel\": url.count('/'),\n",
    "        \"UrlLength\": len(url),\n",
    "        \"NumDash\": url.count('-'),\n",
    "        \"NumDashInHostname\": parsed_url.hostname.count('-'),\n",
    "        \"AtSymbol\": 1 if '@' in url else 0,\n",
    "        \"TildeSymbol\": 1 if '~' in url else 0,\n",
    "        \"NumUnderscore\": url.count('_'),\n",
    "        \"NumPercent\": url.count('%'),\n",
    "        \n",
    "        \"NumQueryComponents\": len(urllib.parse.parse_qs(parsed_url.query)),\n",
    "        \"NumAmpersand\": url.count('&'),\n",
    "        \"NumHash\": url.count('#'),\n",
    "        \"NumNumericChars\": sum(c.isdigit() for c in url),\n",
    "        \"NoHttps\": 1 if parsed_url.scheme != 'https' else 0,\n",
    "        \n",
    "        \"RandomString\": 1 if re.search(r'\\b[0-9a-f]{10}\\b', url) else 0, # Assuming random string contains 10 hex characters\n",
    "        \"IpAddress\": 1 if re.match(r'\\b\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\b', parsed_url.netloc) else 0,\n",
    "        \"DomainInSubdomains\": 1 if parsed_url.netloc.count('.') > 2 else 0,\n",
    "        \"DomainInPaths\": 1 if '.' in parsed_url.path else 0,\n",
    "        \"HttpsInHostname\": 1 if 'https' in parsed_url.netloc else 0,\n",
    "\n",
    "        \"HostnameLength\": len(parsed_url.hostname),\n",
    "        \"PathLength\": len(parsed_url.path),\n",
    "        \"QueryLength\": len(parsed_url.query),\n",
    "        \"DoubleSlashInPath\": 1 if '//' in url else 0,\n",
    "        \n",
    "        \"NumSensitiveWords\": num_sensitive_words,\n",
    "        \"EmbeddedBrandName\": 1 if embedded_brand_name else 0,\n",
    "\n",
    "        \"PctExtHyperlinks\": pct_ext_hyperlinks,\n",
    "        \"PctExtResourceUrls\": pct_ext_resources,\n",
    "        \"ExtFavicon\": ext_favicon,\n",
    "        \"InsecureForms\": insecure_forms,\n",
    "\n",
    "        \"RelativeFormAction\": pct_relative_form_action,\n",
    "        \"ExtFormAction\": pct_ext_form_action,\n",
    "        \"AbnormalFormAction\": pct_abnormal_form_action,\n",
    "        \"PctNullSelfRedirectHyperlinks\": pct_null_self_redirect,\n",
    "\n",
    "        \"FrequentDomainNameMismatch\": frequent_domain_name_mismatch,\n",
    "        \"FakeLinkInStatusBar\": fake_link_in_status_bar,\n",
    "        \"RightClickDisabled\": right_click_disabled,\n",
    "        \"PopUpWindow\": pop_up_window,\n",
    "        \n",
    "    }\n",
    "    \n",
    "    return features\n",
    "\n",
    "# Example usage\n",
    "url = \"https://www.google.com\"\n",
    "features = extract_features(url)\n",
    "print(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "def extract_features(url):\n",
    "    # Function to parse HTML content of the webpage and extract required features\n",
    "    def parse_html_content(html_content):\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "        # Check for frequent domain name mismatch\n",
    "        domain = urllib.parse.urlparse(url).netloc\n",
    "        mismatch_count = sum(1 for link in soup.find_all('a') if domain not in link.get('href'))\n",
    "        frequent_domain_name_mismatch = 1 if mismatch_count > 5 else 0\n",
    "        \n",
    "        # Check for fake links in the status bar\n",
    "        fake_links = sum(1 for link in soup.find_all('a') if 'javascript' in link.get('href', '').lower())\n",
    "        fake_link_in_status_bar = 1 if fake_links > 0 else 0\n",
    "        \n",
    "        # Check if right-click is disabled\n",
    "        right_click_disabled = 1 if 'contextmenu' in html_content.decode().lower() else 0\n",
    "        \n",
    "        # Check for pop-up windows\n",
    "        pop_up_windows = sum(1 for tag in soup.find_all() if tag.get('onload') and 'window.open' in tag.get('onload').lower())\n",
    "        pop_up_window = 1 if pop_up_windows > 0 else 0\n",
    "        \n",
    "        forms = soup.find_all('form')\n",
    "        links = soup.find_all('a')\n",
    "        \n",
    "        total_forms = len(forms)\n",
    "        total_links = len(links)\n",
    "        \n",
    "        # Initialize feature counts\n",
    "        relative_form_action_count = 0\n",
    "        ext_form_action_count = 0\n",
    "        abnormal_form_action_count = 0\n",
    "        null_self_redirect_count = 0\n",
    "        \n",
    "        for form in forms:\n",
    "            action = form.get('action', '')\n",
    "            if action.startswith('/'):\n",
    "                relative_form_action_count += 1\n",
    "            elif '://' in action:\n",
    "                ext_form_action_count += 1\n",
    "            elif action and not action.startswith('#') and not action.startswith('javascript:'):\n",
    "                abnormal_form_action_count += 1\n",
    "        \n",
    "        for link in links:\n",
    "            href = link.get('href', '')\n",
    "            if not href or href.startswith('#') or href.lower() in ['null', 'void(0)']:\n",
    "                null_self_redirect_count += 1\n",
    "        \n",
    "        # Calculate percentages\n",
    "        pct_relative_form_action = (relative_form_action_count / total_forms) * 100 if total_forms > 0 else 0\n",
    "        pct_ext_form_action = (ext_form_action_count / total_forms) * 100 if total_forms > 0 else 0\n",
    "        pct_abnormal_form_action = (abnormal_form_action_count / total_forms) * 100 if total_forms > 0 else 0\n",
    "        pct_null_self_redirect = (null_self_redirect_count / total_links) * 100 if total_links > 0 else 0\n",
    "        \n",
    "        # Count total number of hyperlinks and external hyperlinks\n",
    "        total_hyperlinks = len(links)\n",
    "        ext_hyperlinks = sum(1 for link in links if 'http' in link.get('href'))\n",
    "        pct_ext_hyperlinks = (ext_hyperlinks / total_hyperlinks) * 100 if total_hyperlinks > 0 else 0\n",
    "        \n",
    "        # Count total number of resource URLs and external resource URLs (like images, scripts)\n",
    "        total_resources = len(soup.find_all(['img', 'script', 'link']))\n",
    "        ext_resources = sum(1 for res in soup.find_all(['img', 'script', 'link']) if 'http' in res.get('src', '') or 'http' in res.get('href', ''))\n",
    "        pct_ext_resources = (ext_resources / total_resources) * 100 if total_resources > 0 else 0\n",
    "        \n",
    "        # Check if the webpage has an external favicon\n",
    "        favicon_url = soup.find('link', rel='shortcut icon')\n",
    "        ext_favicon = 1 if favicon_url and 'http' in favicon_url.get('href', '') else 0\n",
    "        \n",
    "        # Check if the webpage contains insecure forms\n",
    "        insecure_forms = any(form.get('action', '').startswith('http://') for form in forms)\n",
    "        \n",
    "        return pct_relative_form_action, pct_ext_form_action, pct_abnormal_form_action, pct_null_self_redirect, pct_ext_hyperlinks, pct_ext_resources, ext_favicon, insecure_forms, frequent_domain_name_mismatch, fake_link_in_status_bar, right_click_disabled, pop_up_window\n",
    "    \n",
    "    # Parse the URL\n",
    "    parsed_url = urllib.parse.urlparse(url)\n",
    "\n",
    "    # Predefined lists of sensitive words and brand names\n",
    "    sensitive_words = [\"login\", \"password\", \"banking\", \"account\", \"verify\", \"secure\"]\n",
    "    brand_names = [\"paypal\", \"google\", \"facebook\", \"amazon\", \"apple\"]\n",
    "\n",
    "    # Function to count occurrences of sensitive words and check for embedded brand names\n",
    "    def count_sensitive_words(url):\n",
    "        num_sensitive_words = sum(1 for word in sensitive_words if word in url.lower())\n",
    "        embedded_brand_name = any(brand in url.lower() for brand in brand_names)\n",
    "        return num_sensitive_words, embedded_brand_name\n",
    "    \n",
    "    # Extract the features\n",
    "    num_sensitive_words, embedded_brand_name = count_sensitive_words(url)\n",
    "    \n",
    "    # Fetch the webpage content\n",
    "    try:\n",
    "        with urllib.request.urlopen(url) as response:\n",
    "            html_content = response.read()\n",
    "            pct_relative_form_action, pct_ext_form_action, pct_abnormal_form_action, pct_null_self_redirect, pct_ext_hyperlinks, pct_ext_resources, ext_favicon, insecure_forms, frequent_domain_name_mismatch, fake_link_in_status_bar, right_click_disabled, pop_up_window = parse_html_content(html_content)\n",
    "    except Exception as e:\n",
    "        print(\"Error fetching webpage:\", e)\n",
    "        pct_relative_form_action, pct_ext_form_action, pct_abnormal_form_action, pct_null_self_redirect, pct_ext_hyperlinks, pct_ext_resources, ext_favicon, insecure_forms, frequent_domain_name_mismatch, fake_link_in_status_bar, right_click_disabled, pop_up_window = 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
    "    \n",
    "    # Features\n",
    "    features = {\n",
    "        \"SubmitInfoToEmail\": 1,  # You need to implement this feature extraction\n",
    "        \"IframeOrFrame\": 1,       # You need to implement this feature extraction\n",
    "        \"MissingTitle\": 1,        # You need to implement this feature extraction\n",
    "        \"ImagesOnlyInForm\": 1,    # You need to implement this feature extraction\n",
    "        \"SubdomainLevelRT\": len(parsed_url.hostname.split('.')),\n",
    "        \"UrlLengthRT\": len(url),\n",
    "        \"PctExtResourceUrlsRT\": pct_ext_resources,\n",
    "        \"AbnormalExtFormActionR\": pct_abnormal_form_action,\n",
    "        \"ExtMetaScriptLinkRT\": pct_ext_hyperlinks,\n",
    "        \"PctExtNullSelfRedirectHyperlinksRT\": pct_null_self_redirect,\n",
    "        \"CLASS_LABEL\": 1          # You need to decide how to determine this label\n",
    "    }\n",
    "    \n",
    "    return features\n",
    "\n",
    "# Example usage\n",
    "url = \"https://www.google.com\"\n",
    "features = extract_features(url)\n",
    "print(features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features extracted from https://google.com : {'SubmitInfoToEmail': False, 'IframeOrFrame': False, 'MissingTitle': False, 'ImagesOnlyInForm': False}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def extract_features_from_url(url):\n",
    "    features = {\n",
    "        \"SubmitInfoToEmail\": False,\n",
    "        \"IframeOrFrame\": False,\n",
    "        \"MissingTitle\": False,\n",
    "        \"ImagesOnlyInForm\": False\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        # Fetch the webpage content\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        # Check if form submits to an email\n",
    "        forms = soup.find_all('form')\n",
    "        for form in forms:\n",
    "            if 'mailto:' in form.get('action', ''):\n",
    "                features[\"SubmitInfoToEmail\"] = True\n",
    "                break\n",
    "\n",
    "        # Check for iframes or frames\n",
    "        if soup.find_all('iframe') or soup.find_all('frame'):\n",
    "            features[\"IframeOrFrame\"] = True\n",
    "\n",
    "        # Check if the webpage has a title\n",
    "        if not soup.title:\n",
    "            features[\"MissingTitle\"] = True\n",
    "\n",
    "        # Check if images are only within form tags\n",
    "        images = soup.find_all('img')\n",
    "        form_images = soup.find_all('form img')\n",
    "        if len(images) == len(form_images):\n",
    "            features[\"ImagesOnlyInForm\"] = True\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Error:\", e)\n",
    "\n",
    "    return features\n",
    "\n",
    "# Example usage:\n",
    "url = \"https://google.com\"\n",
    "features = extract_features_from_url(url)\n",
    "print(\"Features extracted from\", url, \":\", features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'NumDots': 2, 'SubdomainLevel': 3, 'PathLevel': 2, 'UrlLength': 22, 'NumDash': 0, 'NumDashInHostname': 0, 'AtSymbol': 0, 'TildeSymbol': 0, 'NumUnderscore': 0, 'NumPercent': 0, 'NumQueryComponents': 0, 'NumAmpersand': 0, 'NumHash': 0, 'NumNumericChars': 0, 'NoHttps': 0, 'RandomString': 0, 'IpAddress': 0, 'DomainInSubdomains': 0, 'DomainInPaths': 0, 'HttpsInHostname': 0, 'HostnameLength': 14, 'PathLength': 0, 'QueryLength': 0, 'DoubleSlashInPath': 1, 'NumSensitiveWords': 0, 'EmbeddedBrandName': 1, 'PctExtHyperlinks': 70.0, 'PctExtResourceUrls': 0.0, 'ExtFavicon': 0, 'InsecureForms': False, 'RelativeFormAction': 100.0, 'ExtFormAction': 0.0, 'AbnormalFormAction': 0.0, 'PctNullSelfRedirectHyperlinks': 0.0, 'FrequentDomainNameMismatch': 1, 'FakeLinkInStatusBar': 0, 'RightClickDisabled': 0, 'PopUpWindow': 0, 'SubmitInfoToEmail': False, 'IframeOrFrame': False, 'MissingTitle': False, 'ImagesOnlyInForm': False}\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "def extract_features(url):\n",
    "    # Function to parse HTML content of the webpage and extract required features\n",
    "    def parse_html_content(html_content):\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "        # Check for frequent domain name mismatch\n",
    "        domain = urllib.parse.urlparse(url).netloc\n",
    "        mismatch_count = sum(1 for link in soup.find_all('a') if domain not in link.get('href'))\n",
    "        frequent_domain_name_mismatch = 1 if mismatch_count > 5 else 0\n",
    "        \n",
    "        # Check for fake links in the status bar\n",
    "        fake_links = sum(1 for link in soup.find_all('a') if 'javascript' in link.get('href', '').lower())\n",
    "        fake_link_in_status_bar = 1 if fake_links > 0 else 0\n",
    "        \n",
    "        # Check if right-click is disabled\n",
    "        right_click_disabled = 1 if 'contextmenu' in html_content.decode().lower() else 0\n",
    "        \n",
    "        # Check for pop-up windows\n",
    "        pop_up_windows = sum(1 for tag in soup.find_all() if tag.get('onload') and 'window.open' in tag.get('onload').lower())\n",
    "        pop_up_window = 1 if pop_up_windows > 0 else 0\n",
    "        \n",
    "        forms = soup.find_all('form')\n",
    "        links = soup.find_all('a')\n",
    "        \n",
    "        total_forms = len(forms)\n",
    "        total_links = len(links)\n",
    "        \n",
    "        # Initialize feature counts\n",
    "        relative_form_action_count = 0\n",
    "        ext_form_action_count = 0\n",
    "        abnormal_form_action_count = 0\n",
    "        null_self_redirect_count = 0\n",
    "        \n",
    "        for form in forms:\n",
    "            action = form.get('action', '')\n",
    "            if action.startswith('/'):\n",
    "                relative_form_action_count += 1\n",
    "            elif '://' in action:\n",
    "                ext_form_action_count += 1\n",
    "            elif action and not action.startswith('#') and not action.startswith('javascript:'):\n",
    "                abnormal_form_action_count += 1\n",
    "        \n",
    "        for link in links:\n",
    "            href = link.get('href', '')\n",
    "            if not href or href.startswith('#') or href.lower() in ['null', 'void(0)']:\n",
    "                null_self_redirect_count += 1\n",
    "        \n",
    "        # Calculate percentages\n",
    "        pct_relative_form_action = (relative_form_action_count / total_forms) * 100 if total_forms > 0 else 0\n",
    "        pct_ext_form_action = (ext_form_action_count / total_forms) * 100 if total_forms > 0 else 0\n",
    "        pct_abnormal_form_action = (abnormal_form_action_count / total_forms) * 100 if total_forms > 0 else 0\n",
    "        pct_null_self_redirect = (null_self_redirect_count / total_links) * 100 if total_links > 0 else 0\n",
    "        \n",
    "        # Count total number of hyperlinks and external hyperlinks\n",
    "        total_hyperlinks = len(links)\n",
    "        ext_hyperlinks = sum(1 for link in links if 'http' in link.get('href'))\n",
    "        pct_ext_hyperlinks = (ext_hyperlinks / total_hyperlinks) * 100 if total_hyperlinks > 0 else 0\n",
    "        \n",
    "        # Count total number of resource URLs and external resource URLs (like images, scripts)\n",
    "        total_resources = len(soup.find_all(['img', 'script', 'link']))\n",
    "        ext_resources = sum(1 for res in soup.find_all(['img', 'script', 'link']) if 'http' in res.get('src', '') or 'http' in res.get('href', ''))\n",
    "        pct_ext_resources = (ext_resources / total_resources) * 100 if total_resources > 0 else 0\n",
    "        \n",
    "        # Check if the webpage has an external favicon\n",
    "        favicon_url = soup.find('link', rel='shortcut icon')\n",
    "        ext_favicon = 1 if favicon_url and 'http' in favicon_url.get('href', '') else 0\n",
    "        \n",
    "        # Check if the webpage contains insecure forms\n",
    "        insecure_forms = any(form.get('action', '').startswith('http://') for form in forms)\n",
    "        \n",
    "        return pct_relative_form_action, pct_ext_form_action, pct_abnormal_form_action, pct_null_self_redirect, pct_ext_hyperlinks, pct_ext_resources, ext_favicon, insecure_forms, frequent_domain_name_mismatch, fake_link_in_status_bar, right_click_disabled, pop_up_window\n",
    "    \n",
    "    # Parse the URL\n",
    "    parsed_url = urllib.parse.urlparse(url)\n",
    "\n",
    "    # Predefined lists of sensitive words and brand names\n",
    "    sensitive_words = [\"login\", \"password\", \"banking\", \"account\", \"verify\", \"secure\"]\n",
    "    brand_names = [\"paypal\", \"google\", \"facebook\", \"amazon\", \"apple\"]\n",
    "\n",
    "    # Function to count occurrences of sensitive words and check for embedded brand names\n",
    "    def count_sensitive_words(url):\n",
    "        num_sensitive_words = sum(1 for word in sensitive_words if word in url.lower())\n",
    "        embedded_brand_name = any(brand in url.lower() for brand in brand_names)\n",
    "        return num_sensitive_words, embedded_brand_name\n",
    "    \n",
    "    # Extract the features\n",
    "    num_sensitive_words, embedded_brand_name = count_sensitive_words(url)\n",
    "    \n",
    "    # Fetch the webpage content\n",
    "    try:\n",
    "        with urllib.request.urlopen(url) as response:\n",
    "            html_content = response.read()\n",
    "            pct_relative_form_action, pct_ext_form_action, pct_abnormal_form_action, pct_null_self_redirect, pct_ext_hyperlinks, pct_ext_resources, ext_favicon, insecure_forms, frequent_domain_name_mismatch, fake_link_in_status_bar, right_click_disabled, pop_up_window = parse_html_content(html_content)\n",
    "    except Exception as e:\n",
    "        print(\"Error fetching webpage:\", e)\n",
    "        pct_relative_form_action, pct_ext_form_action, pct_abnormal_form_action, pct_null_self_redirect, pct_ext_hyperlinks, pct_ext_resources, ext_favicon, insecure_forms, frequent_domain_name_mismatch, fake_link_in_status_bar, right_click_disabled, pop_up_window = 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
    "    \n",
    "    # Features\n",
    "    features = {\n",
    "        \"NumDots\": url.count('.'),\n",
    "        \"SubdomainLevel\": len(parsed_url.hostname.split('.')),\n",
    "        \"PathLevel\": url.count('/'),\n",
    "        \"UrlLength\": len(url),\n",
    "        \"NumDash\": url.count('-'),\n",
    "        \"NumDashInHostname\": parsed_url.hostname.count('-'),\n",
    "        \"AtSymbol\": 1 if '@' in url else 0,\n",
    "        \"TildeSymbol\": 1 if '~' in url else 0,\n",
    "        \"NumUnderscore\": url.count('_'),\n",
    "        \"NumPercent\": url.count('%'),\n",
    "        \n",
    "        \"NumQueryComponents\": len(urllib.parse.parse_qs(parsed_url.query)),\n",
    "        \"NumAmpersand\": url.count('&'),\n",
    "        \"NumHash\": url.count('#'),\n",
    "        \"NumNumericChars\": sum(c.isdigit() for c in url),\n",
    "        \"NoHttps\": 1 if parsed_url.scheme != 'https' else 0,\n",
    "        \n",
    "        \"RandomString\": 1 if re.search(r'\\b[0-9a-f]{10}\\b', url) else 0, # Assuming random string contains 10 hex characters\n",
    "        \"IpAddress\": 1 if re.match(r'\\b\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\b', parsed_url.netloc) else 0,\n",
    "        \"DomainInSubdomains\": 1 if parsed_url.netloc.count('.') > 2 else 0,\n",
    "        \"DomainInPaths\": 1 if '.' in parsed_url.path else 0,\n",
    "        \"HttpsInHostname\": 1 if 'https' in parsed_url.netloc else 0,\n",
    "\n",
    "        \"HostnameLength\": len(parsed_url.hostname),\n",
    "        \"PathLength\": len(parsed_url.path),\n",
    "        \"QueryLength\": len(parsed_url.query),\n",
    "        \"DoubleSlashInPath\": 1 if '//' in url else 0,\n",
    "        \n",
    "        \"NumSensitiveWords\": num_sensitive_words,\n",
    "        \"EmbeddedBrandName\": 1 if embedded_brand_name else 0,\n",
    "\n",
    "        \"PctExtHyperlinks\": pct_ext_hyperlinks,\n",
    "        \"PctExtResourceUrls\": pct_ext_resources,\n",
    "        \"ExtFavicon\": ext_favicon,\n",
    "        \"InsecureForms\": insecure_forms,\n",
    "\n",
    "        \"RelativeFormAction\": pct_relative_form_action,\n",
    "        \"ExtFormAction\": pct_ext_form_action,\n",
    "        \"AbnormalFormAction\": pct_abnormal_form_action,\n",
    "        \"PctNullSelfRedirectHyperlinks\": pct_null_self_redirect,\n",
    "\n",
    "        \"FrequentDomainNameMismatch\": frequent_domain_name_mismatch,\n",
    "        \"FakeLinkInStatusBar\": fake_link_in_status_bar,\n",
    "        \"RightClickDisabled\": right_click_disabled,\n",
    "        \"PopUpWindow\": pop_up_window,\n",
    "        \n",
    "        \"SubmitInfoToEmail\": False,\n",
    "        \"IframeOrFrame\": False,\n",
    "        \"MissingTitle\": False,\n",
    "        \"ImagesOnlyInForm\": False\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        # Fetch the webpage content\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        # Check if form submits to an email\n",
    "        forms = soup.find_all('form')\n",
    "        for form in forms:\n",
    "            if 'mailto:' in form.get('action', ''):\n",
    "                features[\"SubmitInfoToEmail\"] = True\n",
    "                break\n",
    "\n",
    "        # Check for iframes or frames\n",
    "        if soup.find_all('iframe') or soup.find_all('frame'):\n",
    "            features[\"IframeOrFrame\"] = True\n",
    "\n",
    "        # Check if the webpage has a title\n",
    "        if not soup.title:\n",
    "            features[\"MissingTitle\"] = True\n",
    "\n",
    "        # Check if images are only within form tags\n",
    "        images = soup.find_all('img')\n",
    "        form_images = soup.find_all('form img')\n",
    "        if len(images) == len(form_images):\n",
    "            features[\"ImagesOnlyInForm\"] = True\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Error:\", e)\n",
    "    \n",
    "    return features\n",
    "\n",
    "# Example usage\n",
    "url = \"https://www.google.com\"\n",
    "features = extract_features(url)\n",
    "print(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'NumDots': 2, 'SubdomainLevel': 3, 'PathLevel': 2, 'UrlLength': 22, 'NumDash': 0, 'NumDashInHostname': 0, 'AtSymbol': 0, 'TildeSymbol': 0, 'NumUnderscore': 0, 'NumPercent': 0, 'NumQueryComponents': 0, 'NumAmpersand': 0, 'NumHash': 0, 'NumNumericChars': 0, 'NoHttps': 0, 'RandomString': 0, 'IpAddress': 0, 'DomainInSubdomains': 0, 'DomainInPaths': 0, 'HttpsInHostname': 0, 'HostnameLength': 14, 'PathLength': 0, 'QueryLength': 0, 'DoubleSlashInPath': 1, 'NumSensitiveWords': 0, 'EmbeddedBrandName': 1, 'PctExtHyperlinks': 70.0, 'PctExtResourceUrls': 0.0, 'ExtFavicon': 0, 'InsecureForms': False, 'RelativeFormAction': 100.0, 'ExtFormAction': 0.0, 'AbnormalFormAction': 0.0, 'PctNullSelfRedirectHyperlinks': 0.0, 'FrequentDomainNameMismatch': 1, 'FakeLinkInStatusBar': 0, 'RightClickDisabled': 0, 'PopUpWindow': 0, 'SubmitInfoToEmail': False, 'IframeOrFrame': False, 'MissingTitle': False, 'ImagesOnlyInForm': False, 'SubdomainLevelRT': 3, 'UrlLengthRT': 22, 'PctExtResourceUrlsRT': None, 'AbnormalExtFormActionR': None}\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "def extract_features(url):\n",
    "    # Function to parse HTML content of the webpage and extract required features\n",
    "    def parse_html_content(html_content):\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "        # Check for frequent domain name mismatch\n",
    "        domain = urllib.parse.urlparse(url).netloc\n",
    "        mismatch_count = sum(1 for link in soup.find_all('a') if domain not in link.get('href'))\n",
    "        frequent_domain_name_mismatch = 1 if mismatch_count > 5 else 0\n",
    "        \n",
    "        # Check for fake links in the status bar\n",
    "        fake_links = sum(1 for link in soup.find_all('a') if 'javascript' in link.get('href', '').lower())\n",
    "        fake_link_in_status_bar = 1 if fake_links > 0 else 0\n",
    "        \n",
    "        # Check if right-click is disabled\n",
    "        right_click_disabled = 1 if 'contextmenu' in html_content.decode().lower() else 0\n",
    "        \n",
    "        # Check for pop-up windows\n",
    "        pop_up_windows = sum(1 for tag in soup.find_all() if tag.get('onload') and 'window.open' in tag.get('onload').lower())\n",
    "        pop_up_window = 1 if pop_up_windows > 0 else 0\n",
    "        \n",
    "        forms = soup.find_all('form')\n",
    "        links = soup.find_all('a')\n",
    "        \n",
    "        total_forms = len(forms)\n",
    "        total_links = len(links)\n",
    "        \n",
    "        # Initialize feature counts\n",
    "        relative_form_action_count = 0\n",
    "        ext_form_action_count = 0\n",
    "        abnormal_form_action_count = 0\n",
    "        null_self_redirect_count = 0\n",
    "        \n",
    "        for form in forms:\n",
    "            action = form.get('action', '')\n",
    "            if action.startswith('/'):\n",
    "                relative_form_action_count += 1\n",
    "            elif '://' in action:\n",
    "                ext_form_action_count += 1\n",
    "            elif action and not action.startswith('#') and not action.startswith('javascript:'):\n",
    "                abnormal_form_action_count += 1\n",
    "        \n",
    "        for link in links:\n",
    "            href = link.get('href', '')\n",
    "            if not href or href.startswith('#') or href.lower() in ['null', 'void(0)']:\n",
    "                null_self_redirect_count += 1\n",
    "        \n",
    "        # Calculate percentages\n",
    "        pct_relative_form_action = (relative_form_action_count / total_forms) * 100 if total_forms > 0 else 0\n",
    "        pct_ext_form_action = (ext_form_action_count / total_forms) * 100 if total_forms > 0 else 0\n",
    "        pct_abnormal_form_action = (abnormal_form_action_count / total_forms) * 100 if total_forms > 0 else 0\n",
    "        pct_null_self_redirect = (null_self_redirect_count / total_links) * 100 if total_links > 0 else 0\n",
    "        \n",
    "        # Count total number of hyperlinks and external hyperlinks\n",
    "        total_hyperlinks = len(links)\n",
    "        ext_hyperlinks = sum(1 for link in links if 'http' in link.get('href'))\n",
    "        pct_ext_hyperlinks = (ext_hyperlinks / total_hyperlinks) * 100 if total_hyperlinks > 0 else 0\n",
    "        \n",
    "        # Count total number of resource URLs and external resource URLs (like images, scripts)\n",
    "        total_resources = len(soup.find_all(['img', 'script', 'link']))\n",
    "        ext_resources = sum(1 for res in soup.find_all(['img', 'script', 'link']) if 'http' in res.get('src', '') or 'http' in res.get('href', ''))\n",
    "        pct_ext_resources = (ext_resources / total_resources) * 100 if total_resources > 0 else 0\n",
    "        \n",
    "        # Check if the webpage has an external favicon\n",
    "        favicon_url = soup.find('link', rel='shortcut icon')\n",
    "        ext_favicon = 1 if favicon_url and 'http' in favicon_url.get('href', '') else 0\n",
    "        \n",
    "        # Check if the webpage contains insecure forms\n",
    "        insecure_forms = any(form.get('action', '').startswith('http://') for form in forms)\n",
    "        \n",
    "        return pct_relative_form_action, pct_ext_form_action, pct_abnormal_form_action, pct_null_self_redirect, pct_ext_hyperlinks, pct_ext_resources, ext_favicon, insecure_forms, frequent_domain_name_mismatch, fake_link_in_status_bar, right_click_disabled, pop_up_window\n",
    "    \n",
    "    # Parse the URL\n",
    "    parsed_url = urllib.parse.urlparse(url)\n",
    "\n",
    "    # Calculate SubdomainLevelRT\n",
    "    subdomain_level_rt = len(parsed_url.hostname.split('.'))\n",
    "\n",
    "    # Calculate UrlLengthRT\n",
    "    url_length_rt = len(url)\n",
    "\n",
    "    # Predefined lists of sensitive words and brand names\n",
    "    sensitive_words = [\"login\", \"password\", \"banking\", \"account\", \"verify\", \"secure\"]\n",
    "    brand_names = [\"paypal\", \"google\", \"facebook\", \"amazon\", \"apple\"]\n",
    "\n",
    "    # Function to count occurrences of sensitive words and check for embedded brand names\n",
    "    def count_sensitive_words(url):\n",
    "        num_sensitive_words = sum(1 for word in sensitive_words if word in url.lower())\n",
    "        embedded_brand_name = any(brand in url.lower() for brand in brand_names)\n",
    "        return num_sensitive_words, embedded_brand_name\n",
    "    \n",
    "    # Extract the features\n",
    "    num_sensitive_words, embedded_brand_name = count_sensitive_words(url)\n",
    "    \n",
    "    # Fetch the webpage content\n",
    "    try:\n",
    "        with urllib.request.urlopen(url) as response:\n",
    "            html_content = response.read()\n",
    "            pct_relative_form_action, pct_ext_form_action, pct_abnormal_form_action, pct_null_self_redirect, pct_ext_hyperlinks, pct_ext_resources, ext_favicon, insecure_forms, frequent_domain_name_mismatch, fake_link_in_status_bar, right_click_disabled, pop_up_window = parse_html_content(html_content)\n",
    "    except Exception as e:\n",
    "        print(\"Error fetching webpage:\", e)\n",
    "        pct_relative_form_action, pct_ext_form_action, pct_abnormal_form_action, pct_null_self_redirect, pct_ext_hyperlinks, pct_ext_resources, ext_favicon, insecure_forms, frequent_domain_name_mismatch, fake_link_in_status_bar, right_click_disabled, pop_up_window = 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
    "    \n",
    "    # Features\n",
    "    features = {\n",
    "        \"NumDots\": url.count('.'),\n",
    "        \"SubdomainLevel\": len(parsed_url.hostname.split('.')),\n",
    "        \"PathLevel\": url.count('/'),\n",
    "        \"UrlLength\": len(url),\n",
    "        \"NumDash\": url.count('-'),\n",
    "        \"NumDashInHostname\": parsed_url.hostname.count('-'),\n",
    "        \"AtSymbol\": 1 if '@' in url else 0,\n",
    "        \"TildeSymbol\": 1 if '~' in url else 0,\n",
    "        \"NumUnderscore\": url.count('_'),\n",
    "        \"NumPercent\": url.count('%'),\n",
    "        \n",
    "        \"NumQueryComponents\": len(urllib.parse.parse_qs(parsed_url.query)),\n",
    "        \"NumAmpersand\": url.count('&'),\n",
    "        \"NumHash\": url.count('#'),\n",
    "        \"NumNumericChars\": sum(c.isdigit() for c in url),\n",
    "        \"NoHttps\": 1 if parsed_url.scheme != 'https' else 0,\n",
    "        \n",
    "        \"RandomString\": 1 if re.search(r'\\b[0-9a-f]{10}\\b', url) else 0, # Assuming random string contains 10 hex characters\n",
    "        \"IpAddress\": 1 if re.match(r'\\b\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\b', parsed_url.netloc) else 0,\n",
    "        \"DomainInSubdomains\": 1 if parsed_url.netloc.count('.') > 2 else 0,\n",
    "        \"DomainInPaths\": 1 if '.' in parsed_url.path else 0,\n",
    "        \"HttpsInHostname\": 1 if 'https' in parsed_url.netloc else 0,\n",
    "\n",
    "        \"HostnameLength\": len(parsed_url.hostname),\n",
    "        \"PathLength\": len(parsed_url.path),\n",
    "        \"QueryLength\": len(parsed_url.query),\n",
    "        \"DoubleSlashInPath\": 1 if '//' in url else 0,\n",
    "        \n",
    "        \"NumSensitiveWords\": num_sensitive_words,\n",
    "        \"EmbeddedBrandName\": 1 if embedded_brand_name else 0,\n",
    "\n",
    "        \"PctExtHyperlinks\": pct_ext_hyperlinks,\n",
    "        \"PctExtResourceUrls\": pct_ext_resources,\n",
    "        \"ExtFavicon\": ext_favicon,\n",
    "        \"InsecureForms\": insecure_forms,\n",
    "\n",
    "        \"RelativeFormAction\": pct_relative_form_action,\n",
    "        \"ExtFormAction\": pct_ext_form_action,\n",
    "        \"AbnormalFormAction\": pct_abnormal_form_action,\n",
    "        \"PctNullSelfRedirectHyperlinks\": pct_null_self_redirect,\n",
    "\n",
    "        \"FrequentDomainNameMismatch\": frequent_domain_name_mismatch,\n",
    "        \"FakeLinkInStatusBar\": fake_link_in_status_bar,\n",
    "        \"RightClickDisabled\": right_click_disabled,\n",
    "        \"PopUpWindow\": pop_up_window,\n",
    "        \n",
    "        \"SubmitInfoToEmail\": False,\n",
    "        \"IframeOrFrame\": False,\n",
    "        \"MissingTitle\": False,\n",
    "        \"ImagesOnlyInForm\": False,\n",
    "\n",
    "        \"SubdomainLevelRT\": subdomain_level_rt,\n",
    "        \"UrlLengthRT\": url_length_rt,\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        # Fetch the webpage content\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        # Check if form submits to an email\n",
    "        forms = soup.find_all('form')\n",
    "        for form in forms:\n",
    "            if 'mailto:' in form.get('action', ''):\n",
    "                features[\"SubmitInfoToEmail\"] = True\n",
    "                break\n",
    "\n",
    "        # Check for iframes or frames\n",
    "        if soup.find_all('iframe') or soup.find_all('frame'):\n",
    "            features[\"IframeOrFrame\"] = True\n",
    "\n",
    "        # Check if the webpage has a title\n",
    "        if not soup.title:\n",
    "            features[\"MissingTitle\"] = True\n",
    "\n",
    "        # Check if images are only within form tags\n",
    "        images = soup.find_all('img')\n",
    "        form_images = soup.find_all('form img')\n",
    "        if len(images) == len(form_images):\n",
    "            features[\"ImagesOnlyInForm\"] = True\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Error:\", e)\n",
    "    \n",
    "    return features\n",
    "\n",
    "# Example usage\n",
    "url = \"https://www.google.com\"\n",
    "features = extract_features(url)\n",
    "print(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'NumDots': 2, 'SubdomainLevel': 3, 'PathLevel': 2, 'UrlLength': 22, 'NumDash': 0, 'NumDashInHostname': 0, 'AtSymbol': 0, 'TildeSymbol': 0, 'NumUnderscore': 0, 'NumPercent': 0, 'NumQueryComponents': 0, 'NumAmpersand': 0, 'NumHash': 0, 'NumNumericChars': 0, 'NoHttps': 0, 'RandomString': 0, 'IpAddress': 0, 'DomainInSubdomains': 0, 'DomainInPaths': 0, 'HttpsInHostname': 0, 'HostnameLength': 14, 'PathLength': 0, 'QueryLength': 0, 'DoubleSlashInPath': 1, 'NumSensitiveWords': 0, 'EmbeddedBrandName': 1, 'PctExtHyperlinks': 70.0, 'PctExtResourceUrls': 0.0, 'ExtFavicon': 0, 'InsecureForms': False, 'RelativeFormAction': 100.0, 'ExtFormAction': 0.0, 'AbnormalFormAction': 0.0, 'PctNullSelfRedirectHyperlinks': 0.0, 'FrequentDomainNameMismatch': 1, 'FakeLinkInStatusBar': 0, 'RightClickDisabled': 0, 'PopUpWindow': 0, 'SubmitInfoToEmail': False, 'IframeOrFrame': False, 'MissingTitle': False, 'ImagesOnlyInForm': False, 'SubdomainLevelRT': 3, 'UrlLengthRT': 22, 'PctExtResourceUrlsRT': 0, 'AbnormalExtFormActionR': 1}\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import requests\n",
    "\n",
    "def extract_features(url):\n",
    "    # Function to parse HTML content of the webpage and extract required features\n",
    "    def parse_html_content(html_content):\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "        # Check for frequent domain name mismatch\n",
    "        domain = urllib.parse.urlparse(url).netloc\n",
    "        mismatch_count = sum(1 for link in soup.find_all('a') if domain not in link.get('href'))\n",
    "        frequent_domain_name_mismatch = 1 if mismatch_count > 5 else 0\n",
    "        \n",
    "        # Check for fake links in the status bar\n",
    "        fake_links = sum(1 for link in soup.find_all('a') if 'javascript' in link.get('href', '').lower())\n",
    "        fake_link_in_status_bar = 1 if fake_links > 0 else 0\n",
    "        \n",
    "        # Check if right-click is disabled\n",
    "        right_click_disabled = 1 if 'contextmenu' in html_content.decode().lower() else 0\n",
    "        \n",
    "        # Check for pop-up windows\n",
    "        pop_up_windows = sum(1 for tag in soup.find_all() if tag.get('onload') and 'window.open' in tag.get('onload').lower())\n",
    "        pop_up_window = 1 if pop_up_windows > 0 else 0\n",
    "        \n",
    "        forms = soup.find_all('form')\n",
    "        links = soup.find_all('a')\n",
    "        \n",
    "        total_forms = len(forms)\n",
    "        total_links = len(links)\n",
    "        \n",
    "        # Initialize feature counts\n",
    "        relative_form_action_count = 0\n",
    "        ext_form_action_count = 0\n",
    "        abnormal_form_action_count = 0\n",
    "        null_self_redirect_count = 0\n",
    "        \n",
    "        for form in forms:\n",
    "            action = form.get('action', '')\n",
    "            if action.startswith('/'):\n",
    "                relative_form_action_count += 1\n",
    "            elif '://' in action:\n",
    "                ext_form_action_count += 1\n",
    "            elif action and not action.startswith('#') and not action.startswith('javascript:'):\n",
    "                abnormal_form_action_count += 1\n",
    "        \n",
    "        for link in links:\n",
    "            href = link.get('href', '')\n",
    "            if not href or href.startswith('#') or href.lower() in ['null', 'void(0)']:\n",
    "                null_self_redirect_count += 1\n",
    "        \n",
    "        # Calculate percentages\n",
    "        pct_relative_form_action = (relative_form_action_count / total_forms) * 100 if total_forms > 0 else 0\n",
    "        pct_ext_form_action = (ext_form_action_count / total_forms) * 100 if total_forms > 0 else 0\n",
    "        pct_abnormal_form_action = (abnormal_form_action_count / total_forms) * 100 if total_forms > 0 else 0\n",
    "        pct_null_self_redirect = (null_self_redirect_count / total_links) * 100 if total_links > 0 else 0\n",
    "        \n",
    "        # Count total number of hyperlinks and external hyperlinks\n",
    "        total_hyperlinks = len(links)\n",
    "        ext_hyperlinks = sum(1 for link in links if 'http' in link.get('href'))\n",
    "        pct_ext_hyperlinks = (ext_hyperlinks / total_hyperlinks) * 100 if total_hyperlinks > 0 else 0\n",
    "        \n",
    "        # Count total number of resource URLs and external resource URLs (like images, scripts)\n",
    "        total_resources = len(soup.find_all(['img', 'script', 'link']))\n",
    "        ext_resources = sum(1 for res in soup.find_all(['img', 'script', 'link']) if 'http' in res.get('src', '') or 'http' in res.get('href', ''))\n",
    "        pct_ext_resources = (ext_resources / total_resources) * 100 if total_resources > 0 else 0\n",
    "        \n",
    "        # Check if the webpage has an external favicon\n",
    "        favicon_url = soup.find('link', rel='shortcut icon')\n",
    "        ext_favicon = 1 if favicon_url and 'http' in favicon_url.get('href', '') else 0\n",
    "        \n",
    "        # Check if the webpage contains insecure forms\n",
    "        insecure_forms = any(form.get('action', '').startswith('http://') for form in forms)\n",
    "        \n",
    "        return pct_relative_form_action, pct_ext_form_action, pct_abnormal_form_action, pct_null_self_redirect, pct_ext_hyperlinks, pct_ext_resources, ext_favicon, insecure_forms, frequent_domain_name_mismatch, fake_link_in_status_bar, right_click_disabled, pop_up_window\n",
    "    \n",
    "    # Parse the URL\n",
    "    parsed_url = urllib.parse.urlparse(url)\n",
    "\n",
    "\n",
    "    # Function to fetch the webpage content and extract external resource URLs\n",
    "    def extract_external_resource_urls(url):\n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            ext_resource_urls = [res.get('src') for res in soup.find_all(['img', 'script', 'link']) if 'http' in res.get('src', '')]\n",
    "            return ext_resource_urls\n",
    "        except Exception as e:\n",
    "            print(\"Error fetching webpage:\", e)\n",
    "            return []\n",
    "\n",
    "    # Function to fetch the webpage content and extract form actions\n",
    "    def extract_form_actions(url):\n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            form_actions = [form.get('action', '') for form in soup.find_all('form')]\n",
    "            return form_actions\n",
    "        except Exception as e:\n",
    "            print(\"Error fetching webpage:\", e)\n",
    "            return []\n",
    "\n",
    "    # Calculate PctExtResourceUrlsRT\n",
    "    ext_resource_urls = extract_external_resource_urls(url)\n",
    "    total_resources = len(ext_resource_urls)\n",
    "    total_urls = len([res for res in ext_resource_urls if urllib.parse.urlparse(res).netloc != parsed_url.netloc])\n",
    "    pct_ext_resource_urls_rt = (total_urls / total_resources) * 100 if total_resources > 0 else 0\n",
    "\n",
    "    # Calculate AbnormalExtFormActionR\n",
    "    form_actions = extract_form_actions(url)\n",
    "    abnormal_ext_form_action_r = sum(1 for action in form_actions if urllib.parse.urlparse(action).netloc != parsed_url.netloc)\n",
    "\n",
    "    # Calculate SubdomainLevelRT\n",
    "    subdomain_level_rt = len(parsed_url.hostname.split('.'))\n",
    "\n",
    "    # Calculate UrlLengthRT\n",
    "    url_length_rt = len(url)\n",
    "\n",
    "    # Predefined lists of sensitive words and brand names\n",
    "    sensitive_words = [\"login\", \"password\", \"banking\", \"account\", \"verify\", \"secure\"]\n",
    "    brand_names = [\"paypal\", \"google\", \"facebook\", \"amazon\", \"apple\"]\n",
    "\n",
    "    # Function to count occurrences of sensitive words and check for embedded brand names\n",
    "    def count_sensitive_words(url):\n",
    "        num_sensitive_words = sum(1 for word in sensitive_words if word in url.lower())\n",
    "        embedded_brand_name = any(brand in url.lower() for brand in brand_names)\n",
    "        return num_sensitive_words, embedded_brand_name\n",
    "    \n",
    "    # Extract the features\n",
    "    num_sensitive_words, embedded_brand_name = count_sensitive_words(url)\n",
    "    \n",
    "    # Fetch the webpage content\n",
    "    try:\n",
    "        with urllib.request.urlopen(url) as response:\n",
    "            html_content = response.read()\n",
    "            pct_relative_form_action, pct_ext_form_action, pct_abnormal_form_action, pct_null_self_redirect, pct_ext_hyperlinks, pct_ext_resources, ext_favicon, insecure_forms, frequent_domain_name_mismatch, fake_link_in_status_bar, right_click_disabled, pop_up_window = parse_html_content(html_content)\n",
    "    except Exception as e:\n",
    "        print(\"Error fetching webpage:\", e)\n",
    "        pct_relative_form_action, pct_ext_form_action, pct_abnormal_form_action, pct_null_self_redirect, pct_ext_hyperlinks, pct_ext_resources, ext_favicon, insecure_forms, frequent_domain_name_mismatch, fake_link_in_status_bar, right_click_disabled, pop_up_window = 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
    "    \n",
    "    # Features\n",
    "    features = {\n",
    "        \"NumDots\": url.count('.'),\n",
    "        \"SubdomainLevel\": len(parsed_url.hostname.split('.')),\n",
    "        \"PathLevel\": url.count('/'),\n",
    "        \"UrlLength\": len(url),\n",
    "        \"NumDash\": url.count('-'),\n",
    "        \"NumDashInHostname\": parsed_url.hostname.count('-'),\n",
    "        \"AtSymbol\": 1 if '@' in url else 0,\n",
    "        \"TildeSymbol\": 1 if '~' in url else 0,\n",
    "        \"NumUnderscore\": url.count('_'),\n",
    "        \"NumPercent\": url.count('%'),\n",
    "        \n",
    "        \"NumQueryComponents\": len(urllib.parse.parse_qs(parsed_url.query)),\n",
    "        \"NumAmpersand\": url.count('&'),\n",
    "        \"NumHash\": url.count('#'),\n",
    "        \"NumNumericChars\": sum(c.isdigit() for c in url),\n",
    "        \"NoHttps\": 1 if parsed_url.scheme != 'https' else 0,\n",
    "        \n",
    "        \"RandomString\": 1 if re.search(r'\\b[0-9a-f]{10}\\b', url) else 0, # Assuming random string contains 10 hex characters\n",
    "        \"IpAddress\": 1 if re.match(r'\\b\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\b', parsed_url.netloc) else 0,\n",
    "        \"DomainInSubdomains\": 1 if parsed_url.netloc.count('.') > 2 else 0,\n",
    "        \"DomainInPaths\": 1 if '.' in parsed_url.path else 0,\n",
    "        \"HttpsInHostname\": 1 if 'https' in parsed_url.netloc else 0,\n",
    "\n",
    "        \"HostnameLength\": len(parsed_url.hostname),\n",
    "        \"PathLength\": len(parsed_url.path),\n",
    "        \"QueryLength\": len(parsed_url.query),\n",
    "        \"DoubleSlashInPath\": 1 if '//' in url else 0,\n",
    "        \n",
    "        \"NumSensitiveWords\": num_sensitive_words,\n",
    "        \"EmbeddedBrandName\": 1 if embedded_brand_name else 0,\n",
    "\n",
    "        \"PctExtHyperlinks\": pct_ext_hyperlinks,\n",
    "        \"PctExtResourceUrls\": pct_ext_resources,\n",
    "        \"ExtFavicon\": ext_favicon,\n",
    "        \"InsecureForms\": insecure_forms,\n",
    "\n",
    "        \"RelativeFormAction\": pct_relative_form_action,\n",
    "        \"ExtFormAction\": pct_ext_form_action,\n",
    "        \"AbnormalFormAction\": pct_abnormal_form_action,\n",
    "        \"PctNullSelfRedirectHyperlinks\": pct_null_self_redirect,\n",
    "\n",
    "        \"FrequentDomainNameMismatch\": frequent_domain_name_mismatch,\n",
    "        \"FakeLinkInStatusBar\": fake_link_in_status_bar,\n",
    "        \"RightClickDisabled\": right_click_disabled,\n",
    "        \"PopUpWindow\": pop_up_window,\n",
    "        \n",
    "        \"SubmitInfoToEmail\": False,\n",
    "        \"IframeOrFrame\": False,\n",
    "        \"MissingTitle\": False,\n",
    "        \"ImagesOnlyInForm\": False,\n",
    "\n",
    "        \"SubdomainLevelRT\": subdomain_level_rt,\n",
    "        \"UrlLengthRT\": url_length_rt,\n",
    "\n",
    "        \"PctExtResourceUrlsRT\": pct_ext_resource_urls_rt,\n",
    "        \"AbnormalExtFormActionR\": abnormal_ext_form_action_r,\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        # Fetch the webpage content\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        # Check if form submits to an email\n",
    "        forms = soup.find_all('form')\n",
    "        for form in forms:\n",
    "            if 'mailto:' in form.get('action', ''):\n",
    "                features[\"SubmitInfoToEmail\"] = True\n",
    "                break\n",
    "\n",
    "        # Check for iframes or frames\n",
    "        if soup.find_all('iframe') or soup.find_all('frame'):\n",
    "            features[\"IframeOrFrame\"] = True\n",
    "\n",
    "        # Check if the webpage has a title\n",
    "        if not soup.title:\n",
    "            features[\"MissingTitle\"] = True\n",
    "\n",
    "        # Check if images are only within form tags\n",
    "        images = soup.find_all('img')\n",
    "        form_images = soup.find_all('form img')\n",
    "        if len(images) == len(form_images):\n",
    "            features[\"ImagesOnlyInForm\"] = True\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Error:\", e)\n",
    "    \n",
    "    return features\n",
    "\n",
    "# Example usage\n",
    "url = \"https://www.google.com\"\n",
    "features = extract_features(url)\n",
    "print(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'NumDots': 2, 'SubdomainLevel': 3, 'PathLevel': 2, 'UrlLength': 22, 'NumDash': 0, 'NumDashInHostname': 0, 'AtSymbol': 0, 'TildeSymbol': 0, 'NumUnderscore': 0, 'NumPercent': 0, 'NumQueryComponents': 0, 'NumAmpersand': 0, 'NumHash': 0, 'NumNumericChars': 0, 'NoHttps': 0, 'RandomString': 0, 'IpAddress': 0, 'DomainInSubdomains': 0, 'DomainInPaths': 0, 'HttpsInHostname': 0, 'HostnameLength': 14, 'PathLength': 0, 'QueryLength': 0, 'DoubleSlashInPath': 1, 'NumSensitiveWords': 0, 'EmbeddedBrandName': 1, 'PctExtHyperlinks': 70.0, 'PctExtResourceUrls': 0.0, 'ExtFavicon': 0, 'InsecureForms': False, 'RelativeFormAction': 100.0, 'ExtFormAction': 0.0, 'AbnormalFormAction': 0.0, 'PctNullSelfRedirectHyperlinks': 0.0, 'FrequentDomainNameMismatch': 1, 'FakeLinkInStatusBar': 0, 'RightClickDisabled': 0, 'PopUpWindow': 0, 'SubmitInfoToEmail': False, 'IframeOrFrame': False, 'MissingTitle': False, 'ImagesOnlyInForm': False, 'SubdomainLevelRT': 3, 'UrlLengthRT': 22, 'PctExtResourceUrlsRT': 0, 'AbnormalExtFormActionR': 1, 'ExtMetaScriptLinkRT': 0, 'PctExtNullSelfRedirectHyperlinksRT': 0.0, 'CLASS_LABEL': 'legitimate'}\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import requests\n",
    "\n",
    "def extract_features(url):\n",
    "    # Function to parse HTML content of the webpage and extract required features\n",
    "    def parse_html_content(html_content):\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "        # Check for frequent domain name mismatch\n",
    "        domain = urllib.parse.urlparse(url).netloc\n",
    "        mismatch_count = sum(1 for link in soup.find_all('a') if domain not in link.get('href'))\n",
    "        frequent_domain_name_mismatch = 1 if mismatch_count > 5 else 0\n",
    "        \n",
    "        # Check for fake links in the status bar\n",
    "        fake_links = sum(1 for link in soup.find_all('a') if 'javascript' in link.get('href', '').lower())\n",
    "        fake_link_in_status_bar = 1 if fake_links > 0 else 0\n",
    "        \n",
    "        # Check if right-click is disabled\n",
    "        right_click_disabled = 1 if 'contextmenu' in html_content.decode().lower() else 0\n",
    "        \n",
    "        # Check for pop-up windows\n",
    "        pop_up_windows = sum(1 for tag in soup.find_all() if tag.get('onload') and 'window.open' in tag.get('onload').lower())\n",
    "        pop_up_window = 1 if pop_up_windows > 0 else 0\n",
    "        \n",
    "        forms = soup.find_all('form')\n",
    "        links = soup.find_all('a')\n",
    "        \n",
    "        total_forms = len(forms)\n",
    "        total_links = len(links)\n",
    "        \n",
    "        # Initialize feature counts\n",
    "        relative_form_action_count = 0\n",
    "        ext_form_action_count = 0\n",
    "        abnormal_form_action_count = 0\n",
    "        null_self_redirect_count = 0\n",
    "        \n",
    "        for form in forms:\n",
    "            action = form.get('action', '')\n",
    "            if action.startswith('/'):\n",
    "                relative_form_action_count += 1\n",
    "            elif '://' in action:\n",
    "                ext_form_action_count += 1\n",
    "            elif action and not action.startswith('#') and not action.startswith('javascript:'):\n",
    "                abnormal_form_action_count += 1\n",
    "        \n",
    "        for link in links:\n",
    "            href = link.get('href', '')\n",
    "            if not href or href.startswith('#') or href.lower() in ['null', 'void(0)']:\n",
    "                null_self_redirect_count += 1\n",
    "        \n",
    "        # Calculate percentages\n",
    "        pct_relative_form_action = (relative_form_action_count / total_forms) * 100 if total_forms > 0 else 0\n",
    "        pct_ext_form_action = (ext_form_action_count / total_forms) * 100 if total_forms > 0 else 0\n",
    "        pct_abnormal_form_action = (abnormal_form_action_count / total_forms) * 100 if total_forms > 0 else 0\n",
    "        pct_null_self_redirect = (null_self_redirect_count / total_links) * 100 if total_links > 0 else 0\n",
    "        \n",
    "        # Count total number of hyperlinks and external hyperlinks\n",
    "        total_hyperlinks = len(links)\n",
    "        ext_hyperlinks = sum(1 for link in links if 'http' in link.get('href'))\n",
    "        pct_ext_hyperlinks = (ext_hyperlinks / total_hyperlinks) * 100 if total_hyperlinks > 0 else 0\n",
    "        \n",
    "        # Count total number of resource URLs and external resource URLs (like images, scripts)\n",
    "        total_resources = len(soup.find_all(['img', 'script', 'link']))\n",
    "        ext_resources = sum(1 for res in soup.find_all(['img', 'script', 'link']) if 'http' in res.get('src', '') or 'http' in res.get('href', ''))\n",
    "        pct_ext_resources = (ext_resources / total_resources) * 100 if total_resources > 0 else 0\n",
    "        \n",
    "        # Check if the webpage has an external favicon\n",
    "        favicon_url = soup.find('link', rel='shortcut icon')\n",
    "        ext_favicon = 1 if favicon_url and 'http' in favicon_url.get('href', '') else 0\n",
    "        \n",
    "        # Check if the webpage contains insecure forms\n",
    "        insecure_forms = any(form.get('action', '').startswith('http://') for form in forms)\n",
    "        \n",
    "        return pct_relative_form_action, pct_ext_form_action, pct_abnormal_form_action, pct_null_self_redirect, pct_ext_hyperlinks, pct_ext_resources, ext_favicon, insecure_forms, frequent_domain_name_mismatch, fake_link_in_status_bar, right_click_disabled, pop_up_window\n",
    "    \n",
    "    # Parse the URL\n",
    "    parsed_url = urllib.parse.urlparse(url)\n",
    "\n",
    "    # Function to fetch the webpage content and extract external meta tags, script sources, and link hrefs\n",
    "    def extract_external_content(url):\n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            ext_meta_tags = [meta['content'] for meta in soup.find_all('meta') if 'http' in meta.get('content', '')]\n",
    "            script_sources = [script['src'] for script in soup.find_all('script') if 'http' in script.get('src', '')]\n",
    "            link_hrefs = [link['href'] for link in soup.find_all('link') if 'http' in link.get('href', '')]\n",
    "            return ext_meta_tags, script_sources, link_hrefs\n",
    "        except Exception as e:\n",
    "            print(\"Error fetching webpage:\", e)\n",
    "            return [], [], []\n",
    "\n",
    "    # Function to calculate the percentage of external null self-redirect hyperlinks\n",
    "    def calculate_pct_ext_null_self_redirect_hyperlinks(url):\n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            total_links = len(soup.find_all('a'))\n",
    "            ext_null_self_redirect_links = sum(1 for link in soup.find_all('a') if link.get('href', '').lower() in ['null', 'void(0)'])\n",
    "            pct_ext_null_self_redirect_hyperlinks_rt = (ext_null_self_redirect_links / total_links) * 100 if total_links > 0 else 0\n",
    "            return pct_ext_null_self_redirect_hyperlinks_rt\n",
    "        except Exception as e:\n",
    "            print(\"Error calculating percentage of external null self-redirect hyperlinks:\", e)\n",
    "            return 0\n",
    "\n",
    "    # Calculate ExtMetaScriptLinkRT\n",
    "    ext_meta_tags, script_sources, link_hrefs = extract_external_content(url)\n",
    "    ext_meta_script_link_rt = len(ext_meta_tags) + len(script_sources) + len(link_hrefs)\n",
    "\n",
    "    # Calculate PctExtNullSelfRedirectHyperlinksRT\n",
    "    pct_ext_null_self_redirect_hyperlinks_rt = calculate_pct_ext_null_self_redirect_hyperlinks(url)\n",
    "\n",
    "    # Set the CLASS_LABEL\n",
    "    # Here, you would define your logic to determine the class label based on the features extracted\n",
    "    # For demonstration purposes, let's set it as 'phishing' if any external meta tags, scripts, or links are found, otherwise 'legitimate'\n",
    "    class_label = 'phishing' if ext_meta_script_link_rt > 0 else 'legitimate'\n",
    "\n",
    "\n",
    "    # Function to fetch the webpage content and extract external resource URLs\n",
    "    def extract_external_resource_urls(url):\n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            ext_resource_urls = [res.get('src') for res in soup.find_all(['img', 'script', 'link']) if 'http' in res.get('src', '')]\n",
    "            return ext_resource_urls\n",
    "        except Exception as e:\n",
    "            print(\"Error fetching webpage:\", e)\n",
    "            return []\n",
    "\n",
    "    # Function to fetch the webpage content and extract form actions\n",
    "    def extract_form_actions(url):\n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            form_actions = [form.get('action', '') for form in soup.find_all('form')]\n",
    "            return form_actions\n",
    "        except Exception as e:\n",
    "            print(\"Error fetching webpage:\", e)\n",
    "            return []\n",
    "\n",
    "    # Calculate PctExtResourceUrlsRT\n",
    "    ext_resource_urls = extract_external_resource_urls(url)\n",
    "    total_resources = len(ext_resource_urls)\n",
    "    total_urls = len([res for res in ext_resource_urls if urllib.parse.urlparse(res).netloc != parsed_url.netloc])\n",
    "    pct_ext_resource_urls_rt = (total_urls / total_resources) * 100 if total_resources > 0 else 0\n",
    "\n",
    "    # Calculate AbnormalExtFormActionR\n",
    "    form_actions = extract_form_actions(url)\n",
    "    abnormal_ext_form_action_r = sum(1 for action in form_actions if urllib.parse.urlparse(action).netloc != parsed_url.netloc)\n",
    "\n",
    "    # Calculate SubdomainLevelRT\n",
    "    subdomain_level_rt = len(parsed_url.hostname.split('.'))\n",
    "\n",
    "    # Calculate UrlLengthRT\n",
    "    url_length_rt = len(url)\n",
    "\n",
    "    # Predefined lists of sensitive words and brand names\n",
    "    sensitive_words = [\"login\", \"password\", \"banking\", \"account\", \"verify\", \"secure\"]\n",
    "    brand_names = [\"paypal\", \"google\", \"facebook\", \"amazon\", \"apple\"]\n",
    "\n",
    "    # Function to count occurrences of sensitive words and check for embedded brand names\n",
    "    def count_sensitive_words(url):\n",
    "        num_sensitive_words = sum(1 for word in sensitive_words if word in url.lower())\n",
    "        embedded_brand_name = any(brand in url.lower() for brand in brand_names)\n",
    "        return num_sensitive_words, embedded_brand_name\n",
    "    \n",
    "    # Extract the features\n",
    "    num_sensitive_words, embedded_brand_name = count_sensitive_words(url)\n",
    "    \n",
    "    # Fetch the webpage content\n",
    "    try:\n",
    "        with urllib.request.urlopen(url) as response:\n",
    "            html_content = response.read()\n",
    "            pct_relative_form_action, pct_ext_form_action, pct_abnormal_form_action, pct_null_self_redirect, pct_ext_hyperlinks, pct_ext_resources, ext_favicon, insecure_forms, frequent_domain_name_mismatch, fake_link_in_status_bar, right_click_disabled, pop_up_window = parse_html_content(html_content)\n",
    "    except Exception as e:\n",
    "        print(\"Error fetching webpage:\", e)\n",
    "        pct_relative_form_action, pct_ext_form_action, pct_abnormal_form_action, pct_null_self_redirect, pct_ext_hyperlinks, pct_ext_resources, ext_favicon, insecure_forms, frequent_domain_name_mismatch, fake_link_in_status_bar, right_click_disabled, pop_up_window = 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
    "    \n",
    "    # Features\n",
    "    features = {\n",
    "        \"NumDots\": url.count('.'),\n",
    "        \"SubdomainLevel\": len(parsed_url.hostname.split('.')),\n",
    "        \"PathLevel\": url.count('/'),\n",
    "        \"UrlLength\": len(url),\n",
    "        \"NumDash\": url.count('-'),\n",
    "        \"NumDashInHostname\": parsed_url.hostname.count('-'),\n",
    "        \"AtSymbol\": 1 if '@' in url else 0,\n",
    "        \"TildeSymbol\": 1 if '~' in url else 0,\n",
    "        \"NumUnderscore\": url.count('_'),\n",
    "        \"NumPercent\": url.count('%'),\n",
    "        \n",
    "        \"NumQueryComponents\": len(urllib.parse.parse_qs(parsed_url.query)),\n",
    "        \"NumAmpersand\": url.count('&'),\n",
    "        \"NumHash\": url.count('#'),\n",
    "        \"NumNumericChars\": sum(c.isdigit() for c in url),\n",
    "        \"NoHttps\": 1 if parsed_url.scheme != 'https' else 0,\n",
    "        \n",
    "        \"RandomString\": 1 if re.search(r'\\b[0-9a-f]{10}\\b', url) else 0, # Assuming random string contains 10 hex characters\n",
    "        \"IpAddress\": 1 if re.match(r'\\b\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\b', parsed_url.netloc) else 0,\n",
    "        \"DomainInSubdomains\": 1 if parsed_url.netloc.count('.') > 2 else 0,\n",
    "        \"DomainInPaths\": 1 if '.' in parsed_url.path else 0,\n",
    "        \"HttpsInHostname\": 1 if 'https' in parsed_url.netloc else 0,\n",
    "\n",
    "        \"HostnameLength\": len(parsed_url.hostname),\n",
    "        \"PathLength\": len(parsed_url.path),\n",
    "        \"QueryLength\": len(parsed_url.query),\n",
    "        \"DoubleSlashInPath\": 1 if '//' in url else 0,\n",
    "        \n",
    "        \"NumSensitiveWords\": num_sensitive_words,\n",
    "        \"EmbeddedBrandName\": 1 if embedded_brand_name else 0,\n",
    "\n",
    "        \"PctExtHyperlinks\": pct_ext_hyperlinks,\n",
    "        \"PctExtResourceUrls\": pct_ext_resources,\n",
    "        \"ExtFavicon\": ext_favicon,\n",
    "        \"InsecureForms\": insecure_forms,\n",
    "\n",
    "        \"RelativeFormAction\": pct_relative_form_action,\n",
    "        \"ExtFormAction\": pct_ext_form_action,\n",
    "        \"AbnormalFormAction\": pct_abnormal_form_action,\n",
    "        \"PctNullSelfRedirectHyperlinks\": pct_null_self_redirect,\n",
    "\n",
    "        \"FrequentDomainNameMismatch\": frequent_domain_name_mismatch,\n",
    "        \"FakeLinkInStatusBar\": fake_link_in_status_bar,\n",
    "        \"RightClickDisabled\": right_click_disabled,\n",
    "        \"PopUpWindow\": pop_up_window,\n",
    "        \n",
    "        \"SubmitInfoToEmail\": False,\n",
    "        \"IframeOrFrame\": False,\n",
    "        \"MissingTitle\": False,\n",
    "        \"ImagesOnlyInForm\": False,\n",
    "\n",
    "        \"SubdomainLevelRT\": subdomain_level_rt,\n",
    "        \"UrlLengthRT\": url_length_rt,\n",
    "\n",
    "        \"PctExtResourceUrlsRT\": pct_ext_resource_urls_rt,\n",
    "        \"AbnormalExtFormActionR\": abnormal_ext_form_action_r,\n",
    "\n",
    "        \"ExtMetaScriptLinkRT\": ext_meta_script_link_rt,\n",
    "        \"PctExtNullSelfRedirectHyperlinksRT\": pct_ext_null_self_redirect_hyperlinks_rt,\n",
    "        \"CLASS_LABEL\": class_label\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        # Fetch the webpage content\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        # Check if form submits to an email\n",
    "        forms = soup.find_all('form')\n",
    "        for form in forms:\n",
    "            if 'mailto:' in form.get('action', ''):\n",
    "                features[\"SubmitInfoToEmail\"] = True\n",
    "                break\n",
    "\n",
    "        # Check for iframes or frames\n",
    "        if soup.find_all('iframe') or soup.find_all('frame'):\n",
    "            features[\"IframeOrFrame\"] = True\n",
    "\n",
    "        # Check if the webpage has a title\n",
    "        if not soup.title:\n",
    "            features[\"MissingTitle\"] = True\n",
    "\n",
    "        # Check if images are only within form tags\n",
    "        images = soup.find_all('img')\n",
    "        form_images = soup.find_all('form img')\n",
    "        if len(images) == len(form_images):\n",
    "            features[\"ImagesOnlyInForm\"] = True\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Error:\", e)\n",
    "    \n",
    "    return features\n",
    "\n",
    "# Example usage\n",
    "url = \"https://www.google.com\"\n",
    "features = extract_features(url)\n",
    "print(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching webpage: HTTP Error 403: Forbidden\n",
      "{'NumDots': 1, 'SubdomainLevel': 2, 'PathLevel': 3, 'UrlLength': 20, 'NumDash': 0, 'NumDashInHostname': 0, 'AtSymbol': 0, 'TildeSymbol': 0, 'NumUnderscore': 0, 'NumPercent': 0, 'NumQueryComponents': 0, 'NumAmpersand': 0, 'NumHash': 0, 'NumNumericChars': 1, 'NoHttps': 0, 'RandomString': 0, 'IpAddress': 0, 'DomainInSubdomains': 0, 'DomainInPaths': 0, 'HttpsInHostname': 0, 'HostnameLength': 11, 'PathLength': 1, 'QueryLength': 0, 'DoubleSlashInPath': 1, 'NumSensitiveWords': 0, 'EmbeddedBrandName': 0, 'PctExtHyperlinks': 0, 'PctExtResourceUrls': 0, 'ExtFavicon': 0, 'InsecureForms': 0, 'RelativeFormAction': 0, 'ExtFormAction': 0, 'AbnormalFormAction': 0, 'PctNullSelfRedirectHyperlinks': 0, 'FrequentDomainNameMismatch': 0, 'FakeLinkInStatusBar': 0, 'RightClickDisabled': 0, 'PopUpWindow': 0, 'SubmitInfoToEmail': False, 'IframeOrFrame': False, 'MissingTitle': False, 'ImagesOnlyInForm': False, 'SubdomainLevelRT': 2, 'UrlLengthRT': 20, 'PctExtResourceUrlsRT': 100.0, 'AbnormalExtFormActionR': 22, 'ExtMetaScriptLinkRT': 26, 'PctExtNullSelfRedirectHyperlinksRT': 0.0, 'CLASS_LABEL': 'phishing'}\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import requests\n",
    "\n",
    "def extract_features(url):\n",
    "    # Function to parse HTML content of the webpage and extract required features\n",
    "    def parse_html_content(html_content):\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "        # Check for frequent domain name mismatch\n",
    "        domain = urllib.parse.urlparse(url).netloc\n",
    "        mismatch_count = sum(1 for link in soup.find_all('a') if domain not in link.get('href'))\n",
    "        frequent_domain_name_mismatch = 1 if mismatch_count > 5 else 0\n",
    "        \n",
    "        # Check for fake links in the status bar\n",
    "        fake_links = sum(1 for link in soup.find_all('a') if 'javascript' in link.get('href', '').lower())\n",
    "        fake_link_in_status_bar = 1 if fake_links > 0 else 0\n",
    "        \n",
    "        # Check if right-click is disabled\n",
    "        right_click_disabled = 1 if 'contextmenu' in html_content.decode().lower() else 0\n",
    "        \n",
    "        # Check for pop-up windows\n",
    "        pop_up_windows = sum(1 for tag in soup.find_all() if tag.get('onload') and 'window.open' in tag.get('onload').lower())\n",
    "        pop_up_window = 1 if pop_up_windows > 0 else 0\n",
    "        \n",
    "        forms = soup.find_all('form')\n",
    "        links = soup.find_all('a')\n",
    "        \n",
    "        total_forms = len(forms)\n",
    "        total_links = len(links)\n",
    "        \n",
    "        # Initialize feature counts\n",
    "        relative_form_action_count = 0\n",
    "        ext_form_action_count = 0\n",
    "        abnormal_form_action_count = 0\n",
    "        null_self_redirect_count = 0\n",
    "        \n",
    "        for form in forms:\n",
    "            action = form.get('action', '')\n",
    "            if action.startswith('/'):\n",
    "                relative_form_action_count += 1\n",
    "            elif '://' in action:\n",
    "                ext_form_action_count += 1\n",
    "            elif action and not action.startswith('#') and not action.startswith('javascript:'):\n",
    "                abnormal_form_action_count += 1\n",
    "        \n",
    "        for link in links:\n",
    "            href = link.get('href', '')\n",
    "            if not href or href.startswith('#') or href.lower() in ['null', 'void(0)']:\n",
    "                null_self_redirect_count += 1\n",
    "        \n",
    "        # Calculate percentages\n",
    "        pct_relative_form_action = (relative_form_action_count / total_forms) * 100 if total_forms > 0 else 0\n",
    "        pct_ext_form_action = (ext_form_action_count / total_forms) * 100 if total_forms > 0 else 0\n",
    "        pct_abnormal_form_action = (abnormal_form_action_count / total_forms) * 100 if total_forms > 0 else 0\n",
    "        pct_null_self_redirect = (null_self_redirect_count / total_links) * 100 if total_links > 0 else 0\n",
    "        \n",
    "        # Count total number of hyperlinks and external hyperlinks\n",
    "        total_hyperlinks = len(links)\n",
    "        ext_hyperlinks = sum(1 for link in links if 'http' in link.get('href'))\n",
    "        pct_ext_hyperlinks = (ext_hyperlinks / total_hyperlinks) * 100 if total_hyperlinks > 0 else 0\n",
    "        \n",
    "        # Count total number of resource URLs and external resource URLs (like images, scripts)\n",
    "        total_resources = len(soup.find_all(['img', 'script', 'link']))\n",
    "        ext_resources = sum(1 for res in soup.find_all(['img', 'script', 'link']) if 'http' in res.get('src', '') or 'http' in res.get('href', ''))\n",
    "        pct_ext_resources = (ext_resources / total_resources) * 100 if total_resources > 0 else 0\n",
    "        \n",
    "        # Check if the webpage has an external favicon\n",
    "        favicon_url = soup.find('link', rel='shortcut icon')\n",
    "        ext_favicon = 1 if favicon_url and 'http' in favicon_url.get('href', '') else 0\n",
    "        \n",
    "        # Check if the webpage contains insecure forms\n",
    "        insecure_forms = any(form.get('action', '').startswith('http://') for form in forms)\n",
    "        \n",
    "        return pct_relative_form_action, pct_ext_form_action, pct_abnormal_form_action, pct_null_self_redirect, pct_ext_hyperlinks, pct_ext_resources, ext_favicon, insecure_forms, frequent_domain_name_mismatch, fake_link_in_status_bar, right_click_disabled, pop_up_window\n",
    "    \n",
    "    # Parse the URL\n",
    "    parsed_url = urllib.parse.urlparse(url)\n",
    "\n",
    "    # Function to fetch the webpage content and extract external meta tags, script sources, and link hrefs\n",
    "    def extract_external_content(url):\n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            ext_meta_tags = [meta['content'] for meta in soup.find_all('meta') if 'http' in meta.get('content', '')]\n",
    "            script_sources = [script['src'] for script in soup.find_all('script') if 'http' in script.get('src', '')]\n",
    "            link_hrefs = [link['href'] for link in soup.find_all('link') if 'http' in link.get('href', '')]\n",
    "            return ext_meta_tags, script_sources, link_hrefs\n",
    "        except Exception as e:\n",
    "            print(\"Error fetching webpage:\", e)\n",
    "            return [], [], []\n",
    "\n",
    "    # Function to calculate the percentage of external null self-redirect hyperlinks\n",
    "    def calculate_pct_ext_null_self_redirect_hyperlinks(url):\n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            total_links = len(soup.find_all('a'))\n",
    "            ext_null_self_redirect_links = sum(1 for link in soup.find_all('a') if link.get('href', '').lower() in ['null', 'void(0)'])\n",
    "            pct_ext_null_self_redirect_hyperlinks_rt = (ext_null_self_redirect_links / total_links) * 100 if total_links > 0 else 0\n",
    "            return pct_ext_null_self_redirect_hyperlinks_rt\n",
    "        except Exception as e:\n",
    "            print(\"Error calculating percentage of external null self-redirect hyperlinks:\", e)\n",
    "            return 0\n",
    "\n",
    "    # Calculate ExtMetaScriptLinkRT\n",
    "    ext_meta_tags, script_sources, link_hrefs = extract_external_content(url)\n",
    "    ext_meta_script_link_rt = len(ext_meta_tags) + len(script_sources) + len(link_hrefs)\n",
    "\n",
    "    # Calculate PctExtNullSelfRedirectHyperlinksRT\n",
    "    pct_ext_null_self_redirect_hyperlinks_rt = calculate_pct_ext_null_self_redirect_hyperlinks(url)\n",
    "\n",
    "    # Set the CLASS_LABEL\n",
    "    # Here, you would define your logic to determine the class label based on the features extracted\n",
    "    # For demonstration purposes, let's set it as 'phishing' if any external meta tags, scripts, or links are found, otherwise 'legitimate'\n",
    "    class_label = 'phishing' if ext_meta_script_link_rt > 0 else 'legitimate'\n",
    "\n",
    "\n",
    "    # Function to fetch the webpage content and extract external resource URLs\n",
    "    def extract_external_resource_urls(url):\n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            ext_resource_urls = [res.get('src') for res in soup.find_all(['img', 'script', 'link']) if 'http' in res.get('src', '')]\n",
    "            return ext_resource_urls\n",
    "        except Exception as e:\n",
    "            print(\"Error fetching webpage:\", e)\n",
    "            return []\n",
    "\n",
    "    # Function to fetch the webpage content and extract form actions\n",
    "    def extract_form_actions(url):\n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            form_actions = [form.get('action', '') for form in soup.find_all('form')]\n",
    "            return form_actions\n",
    "        except Exception as e:\n",
    "            print(\"Error fetching webpage:\", e)\n",
    "            return []\n",
    "\n",
    "    # Calculate PctExtResourceUrlsRT\n",
    "    ext_resource_urls = extract_external_resource_urls(url)\n",
    "    total_resources = len(ext_resource_urls)\n",
    "    total_urls = len([res for res in ext_resource_urls if urllib.parse.urlparse(res).netloc != parsed_url.netloc])\n",
    "    pct_ext_resource_urls_rt = (total_urls / total_resources) * 100 if total_resources > 0 else 0\n",
    "\n",
    "    # Calculate AbnormalExtFormActionR\n",
    "    form_actions = extract_form_actions(url)\n",
    "    abnormal_ext_form_action_r = sum(1 for action in form_actions if urllib.parse.urlparse(action).netloc != parsed_url.netloc)\n",
    "\n",
    "    # Calculate SubdomainLevelRT\n",
    "    subdomain_level_rt = len(parsed_url.hostname.split('.'))\n",
    "\n",
    "    # Calculate UrlLengthRT\n",
    "    url_length_rt = len(url)\n",
    "\n",
    "    # Predefined lists of sensitive words and brand names\n",
    "    sensitive_words = [\"login\", \"password\", \"banking\", \"account\", \"verify\", \"secure\"]\n",
    "    brand_names = [\"paypal\", \"google\", \"facebook\", \"amazon\", \"apple\"]\n",
    "\n",
    "    # Function to count occurrences of sensitive words and check for embedded brand names\n",
    "    def count_sensitive_words(url):\n",
    "        num_sensitive_words = sum(1 for word in sensitive_words if word in url.lower())\n",
    "        embedded_brand_name = any(brand in url.lower() for brand in brand_names)\n",
    "        return num_sensitive_words, embedded_brand_name\n",
    "    \n",
    "    # Extract the features\n",
    "    num_sensitive_words, embedded_brand_name = count_sensitive_words(url)\n",
    "    \n",
    "    # Fetch the webpage content\n",
    "    try:\n",
    "        with urllib.request.urlopen(url) as response:\n",
    "            html_content = response.read()\n",
    "            pct_relative_form_action, pct_ext_form_action, pct_abnormal_form_action, pct_null_self_redirect, pct_ext_hyperlinks, pct_ext_resources, ext_favicon, insecure_forms, frequent_domain_name_mismatch, fake_link_in_status_bar, right_click_disabled, pop_up_window = parse_html_content(html_content)\n",
    "    except Exception as e:\n",
    "        print(\"Error fetching webpage:\", e)\n",
    "        pct_relative_form_action, pct_ext_form_action, pct_abnormal_form_action, pct_null_self_redirect, pct_ext_hyperlinks, pct_ext_resources, ext_favicon, insecure_forms, frequent_domain_name_mismatch, fake_link_in_status_bar, right_click_disabled, pop_up_window = 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
    "    \n",
    "    # Features\n",
    "    features = {\n",
    "        \"NumDots\": url.count('.'),\n",
    "        \"SubdomainLevel\": len(parsed_url.hostname.split('.')),\n",
    "        \"PathLevel\": url.count('/'),\n",
    "        \"UrlLength\": len(url),\n",
    "        \"NumDash\": url.count('-'),\n",
    "        \"NumDashInHostname\": parsed_url.hostname.count('-'),\n",
    "        \"AtSymbol\": 1 if '@' in url else 0,\n",
    "        \"TildeSymbol\": 1 if '~' in url else 0,\n",
    "        \"NumUnderscore\": url.count('_'),\n",
    "        \"NumPercent\": url.count('%'),\n",
    "        \n",
    "        \"NumQueryComponents\": len(urllib.parse.parse_qs(parsed_url.query)),\n",
    "        \"NumAmpersand\": url.count('&'),\n",
    "        \"NumHash\": url.count('#'),\n",
    "        \"NumNumericChars\": sum(c.isdigit() for c in url),\n",
    "        \"NoHttps\": 1 if parsed_url.scheme != 'https' else 0,\n",
    "        \n",
    "        \"RandomString\": 1 if re.search(r'\\b[0-9a-f]{10}\\b', url) else 0, # Assuming random string contains 10 hex characters\n",
    "        \"IpAddress\": 1 if re.match(r'\\b\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\b', parsed_url.netloc) else 0,\n",
    "        \"DomainInSubdomains\": 1 if parsed_url.netloc.count('.') > 2 else 0,\n",
    "        \"DomainInPaths\": 1 if '.' in parsed_url.path else 0,\n",
    "        \"HttpsInHostname\": 1 if 'https' in parsed_url.netloc else 0,\n",
    "\n",
    "        \"HostnameLength\": len(parsed_url.hostname),\n",
    "        \"PathLength\": len(parsed_url.path),\n",
    "        \"QueryLength\": len(parsed_url.query),\n",
    "        \"DoubleSlashInPath\": 1 if '//' in url else 0,\n",
    "        \n",
    "        \"NumSensitiveWords\": num_sensitive_words,\n",
    "        \"EmbeddedBrandName\": 1 if embedded_brand_name else 0,\n",
    "\n",
    "        \"PctExtHyperlinks\": pct_ext_hyperlinks,\n",
    "        \"PctExtResourceUrls\": pct_ext_resources,\n",
    "        \"ExtFavicon\": ext_favicon,\n",
    "        \"InsecureForms\": insecure_forms,\n",
    "\n",
    "        \"RelativeFormAction\": pct_relative_form_action,\n",
    "        \"ExtFormAction\": pct_ext_form_action,\n",
    "        \"AbnormalFormAction\": pct_abnormal_form_action,\n",
    "        \"PctNullSelfRedirectHyperlinks\": pct_null_self_redirect,\n",
    "\n",
    "        \"FrequentDomainNameMismatch\": frequent_domain_name_mismatch,\n",
    "        \"FakeLinkInStatusBar\": fake_link_in_status_bar,\n",
    "        \"RightClickDisabled\": right_click_disabled,\n",
    "        \"PopUpWindow\": pop_up_window,\n",
    "        \n",
    "        \"SubmitInfoToEmail\": False,\n",
    "        \"IframeOrFrame\": False,\n",
    "        \"MissingTitle\": False,\n",
    "        \"ImagesOnlyInForm\": False,\n",
    "\n",
    "        \"SubdomainLevelRT\": subdomain_level_rt,\n",
    "        \"UrlLengthRT\": url_length_rt,\n",
    "\n",
    "        \"PctExtResourceUrlsRT\": pct_ext_resource_urls_rt,\n",
    "        \"AbnormalExtFormActionR\": abnormal_ext_form_action_r,\n",
    "\n",
    "        \"ExtMetaScriptLinkRT\": ext_meta_script_link_rt,\n",
    "        \"PctExtNullSelfRedirectHyperlinksRT\": pct_ext_null_self_redirect_hyperlinks_rt,\n",
    "        \"CLASS_LABEL\": class_label\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        # Fetch the webpage content\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        # Check if form submits to an email\n",
    "        forms = soup.find_all('form')\n",
    "        for form in forms:\n",
    "            if 'mailto:' in form.get('action', ''):\n",
    "                features[\"SubmitInfoToEmail\"] = True\n",
    "                break\n",
    "\n",
    "        # Check for iframes or frames\n",
    "        if soup.find_all('iframe') or soup.find_all('frame'):\n",
    "            features[\"IframeOrFrame\"] = True\n",
    "\n",
    "        # Check if the webpage has a title\n",
    "        if not soup.title:\n",
    "            features[\"MissingTitle\"] = True\n",
    "\n",
    "        # Check if images are only within form tags\n",
    "        images = soup.find_all('img')\n",
    "        form_images = soup.find_all('form img')\n",
    "        if len(images) == len(form_images):\n",
    "            features[\"ImagesOnlyInForm\"] = True\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Error:\", e)\n",
    "    \n",
    "    return features\n",
    "\n",
    "# Example usage\n",
    "url = \"https://cults3d.com/\"\n",
    "features = extract_features(url)\n",
    "print(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
